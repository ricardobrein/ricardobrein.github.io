{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hi there \ud83d\udc4b Gracias por visitar mi blog personal. Bienvenido!","text":"<p> Machine Learning | AI | Big data | Junior Developer </p> <p> </p>"},{"location":"#about-me","title":"About me","text":"<p>Ricardo enjoys working with <code>data, statistics, and applied Machine Learning</code> to tackle real-world business and societal problems. His goal is to build a professional path as a Full-stack Data Scientist, understanding the entire lifecycle of an end-to-end ML system.   </p> <p>He is committed to:</p> <ul> <li>Self-taught continous learning  </li> <li>Spread the word, Adding value to the AI community in Espa\u00f1ol coming soon: Youtube channel</li> <li>Crafting solutions that better our world, applying ML ethically and responsibly.</li> </ul> <p>When he's keyboard isn't clacking away, you'll find him with Le\u00f3n\ud83e\udd81, walking or rambling in the analog world.</p>"},{"location":"#interests","title":"Interests","text":"<ul> <li>\ud83d\udcbb Python programming, Data, applied AI</li> <li>\ud83e\uddd9\ud83c\udffb\u200d\u2642\ufe0f MLOps, CI/CD, deployment</li> <li>\ud83e\udde0 Machine / Deep Learning with scikit-learn, pytorch, tensorflow</li> <li>\u26d3\ufe0f Security and reliability</li> <li>\u2601\ufe0f Cloud computing, Big Data architecture in Azure, AWS, GCP.</li> <li></li> </ul>"},{"location":"#get-in-touch","title":"Get in touch.","text":"<p>Phone  Mail </p> <p>More about me  Stack   CV </p>"},{"location":"#blogs","title":"Blogs","text":"blogs sobre mis aprendizajes y otros temas. <p>Jupyter Notebooks para practicar deliberadamente y compartir ejemplos pr\u00e1cticos.</p> <p>Learning Blog\ud83d\uddd2\ufe0f Hashnode  Medium </p>"},{"location":"#currently-reading-learning","title":"Currently Reading &amp; Learning \ud83d\udcda","text":"<ul> <li>More scraping </li> <li>Containerization with docker</li> <li>MlOps at scale.</li> </ul>"},{"location":"about/","title":"Acerca de Mi","text":"<p>Ricardo, es un desarrollador Junior de IA, Machine learning y big data, impulsado por la curiosidad y por las ganas de participar en la creaci\u00f3n de soluciones aprovechando el impacto que tiene las t\u00e9cnicas de <code>Inteligencia Artificial</code> en muchos sectores de la sociedad, la industria y en la manera en como nos relacionamos con la tecnolog\u00eda.</p> <p>Creo en la posibilidad de ser polivalente y generalista, abarcando cada aspecto de una canalizaci\u00f3n de datos, desde que se produce hasta que se introduce en el bucle de , convertirlo en conocimiento. Ense\u00f1ar a la maquina y sacar conclusiones mediante an\u00e1lisis estad\u00edstico o un modelo de Machine learning que haga inferencias sobre ellos.</p>"},{"location":"about/#skills-interest","title":"Skills | Interest","text":"<p>\ud83d\udcbb Datos e IA aplicada a la empresa: Python, shell, SQL, Polars, Matplotlib, Modelos de predicci\u00f3n, Redes Neuronales, NLP, CV, Power BI</p> <p>\ud83e\uddd9\ud83c\udffb\u200d\u2642\ufe0f MLOps | CI/CD: GIT, Pytest, Github Actions, </p> <p>\ud83e\udde0 Machine / Deep Learning: Scikit Learn, Amazon Sagemaker, Tensorflow, Keras, MXnet,  arquitectura de redes neuronales como Conv, Recurrent, LSTMs, Transformers y como hacerlas m\u00e1s eficientes.</p> <p>\u2601\ufe0f Cloud computing, Big Data: AWS, Azure, Kafka, Flink, airflow, Pyspark, FastAPI.</p> <p>\u26d3\ufe0f Ciberscurity</p>"},{"location":"about/#estudios","title":"Estudios:","text":"<p>Estudio mayormente de manera autodidacta, mediante libros, haciendo casos pr\u00e1cticos, certificaciones financiadas por la UE y otros cursos en plataformas online como Udemy, Microsoft Learn y Coursera.</p> <p>Certificaciones </p>"},{"location":"certifications/","title":"Certificados","text":""},{"location":"certifications/#mlops-con-microsoft-azure-ml-y-github-actions","title":"<code>MLOps con Microsoft Azure ML y Github Actions</code>","text":""},{"location":"certifications/#modelos-y-sistemas-de-inteligencia-artificial-basados-en-aprendizaje-automatico","title":"<code>Modelos y sistemas de Inteligencia Artificial basados en aprendizaje autom\u00e1tico</code>","text":""},{"location":"certifications/#machine-learning-aplicado-usando-python","title":"<code>Machine Learning Aplicado Usando Python</code>","text":""},{"location":"certifications/#experto-en-data-analytics","title":"<code>Experto en Data &amp; Analytics</code>","text":""},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#proyectos-con-los-que-aprendo","title":"Proyectos con los que aprendo.","text":""},{"location":"stack/","title":"Tech Stack","text":""},{"location":"stack/#otras-herramientas","title":"Otras herramientas","text":""},{"location":"blog/blog/","title":"Blog","text":""},{"location":"blog/blog/#_1","title":"Blog","text":"Tip: Si el c\u00f3digo te parece demasiado peque\u00f1o... <p>Te sugiero hacerle <code>ZOOM</code> a tu navegador, hasta 125% - 150% para una mejor experiencia visual</p>"},{"location":"blog/blog/#state-of-the-art-of-machine-learning-systems","title":"State of the art of machine learning systems","text":"<p>SOTA de los sistemas de machine learning, haremos un an\u00e1lisis de la actualidad relacionada con el ml</p> <p>Continue Reading \u2794</p>"},{"location":"blog/blog/#causal-impact","title":"Causal Impact","text":"<p> CausalImpact creado por Google estima el impacto de una intervenci\u00f3n en una serie temporal. Continue reading \u2794</p>"},{"location":"blog/blog/#collaborative-filtering","title":"Collaborative Filtering","text":"<p> User-based collaborative filtering para realizar un mejor sistema de recomendaci\u00f3n de pel\u00edculas.  Continue reading \u2794](2022/2022-10-12-implicit.ipynb)</p>"},{"location":"blog/blog/#test-driven-development","title":"Test Driven Development","text":"<p>[] C\u00f3mo abordar el desarrollo de software para Data Science usando Test Driven Development.  Continue reading \u2794</p>"},{"location":"blog/blog/#polars","title":"Polars","text":"<p> Polars es una librer\u00eda de DataFrames incre\u00edblemente r\u00e1pida y eficiente  implementada en Rust. Continue reading \u2794</p>"},{"location":"blog/blog/#impact-on-digital-learning","title":"Impact on Digital Learning","text":"<p> Competition Solution:  LearnPlatform  COVID-19 Impact on Digital Learning proposed by Kaggle. Continue reading \u2794 </p>"},{"location":"blog/blog/#buenas-practicas","title":"Buenas Pr\u00e1cticas","text":"<p> Consejos que te ayudar\u00e1n a mejorar tus skills en el desarrollo de software (con Python). Continue reading \u2794</p>"},{"location":"blog/blog/#jupyter-book","title":"Jupyter Book","text":"<p> Jupyter Book es una herramienta para crear documentos mediante Jupyter Notebooks y/o Markdown. Continue reading \u2794 </p>"},{"location":"blog/blog/#rise","title":"RISE","text":"<p> RISE es una extensi\u00f3n a los Jupyter Notebooks que permite transformar  tus notebooks en presentaciones interactivas. Continue reading \u2794 </p>"},{"location":"blog/blog/#jupyter-noteboook","title":"Jupyter Noteboook","text":"<p> Jupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar c\u00f3digo en Python. Continue reading \u2794</p>"},{"location":"blog/2021/2021-07-31-jupyter/","title":"Jupyter Noteboook","text":"<p>A continuaci\u00f3n, abrir\u00e1 su navegador web predeterminado a esta URL. Cuando el notebook se abra en su navegador, ver\u00e1 el Panel, que mostrar\u00e1 una lista de notebooks, archivos y subdirectorios en el directorio donde se inici\u00f3 el servidor.</p> <p></p> <p>La parte superior de la lista de notebooks se muestran rutas de navegaci\u00f3n en las que se puede hacer clic del directorio actual.</p> <p>Para crear un nuevo notebook, haga clic en el bot\u00f3n <code>New</code> en la parte superior de la lista y seleccione el kernel del men\u00fa desplegable (como se ve a continuaci\u00f3n). Los kernels que se enumeran dependen de lo que est\u00e9 instalado en el servidor.</p> <p>Nota: Es posible que algunos de los kernels de la siguiente captura de pantalla no existan como una opci\u00f3n para usted.</p> <p></p> <p>Una vez seleccionado el kernel, se abrira nuestro primer notebook!.</p> <p></p> <p>$$p(x) = 3x^2 + 5y^2 + x^2y^2$$</p> <p>$$e^{\\pi i} - 1 = 0$$</p> <p>$$\\lim_{x \\rightarrow \\infty} 3x+1$$</p> <p>$$\\sum_{n=1}^\\infty\\frac{1}{n^2}$$</p> <p>$$\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}$$</p> In\u00a0[1]: Copied! <pre>import math\nn = 16\nprint(f\"La raiz cuadra de {n} es {math.sqrt(n)}\")\n</pre> import math n = 16 print(f\"La raiz cuadra de {n} es {math.sqrt(n)}\") <pre>La raiz cuadra de 16 es 4.0\n</pre> <p>Tambi\u00e9n es posible visualizar tablas de datos con la librer\u00eda <code>pandas</code>:</p> In\u00a0[2]: Copied! <pre>#collapse-hide\nimport pandas as pd\nimport altair as alt\n\n# datasets\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\n</pre> #collapse-hide import pandas as pd import altair as alt  # datasets movies = 'https://vega.github.io/vega-datasets/data/movies.json' stocks = 'https://vega.github.io/vega-datasets/data/stocks.csv' In\u00a0[3]: Copied! <pre>df = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\n\ndf.head()\n</pre> df = pd.read_json(movies) # load movies data df.columns = [x.replace(' ', '_') for x in df.columns.values]  df.head() Out[3]: Title US_Gross Worldwide_Gross US_DVD_Sales Production_Budget Release_Date MPAA_Rating Running_Time_min Distributor Source Major_Genre Creative_Type Director Rotten_Tomatoes_Rating IMDB_Rating IMDB_Votes 0 The Land Girls 146083.0 146083.0 NaN 8000000.0 Jun 12 1998 R NaN Gramercy None None None None NaN 6.1 1071.0 1 First Love, Last Rites 10876.0 10876.0 NaN 300000.0 Aug 07 1998 R NaN Strand None Drama None None NaN 6.9 207.0 2 I Married a Strange Person 203134.0 203134.0 NaN 250000.0 Aug 28 1998 None NaN Lionsgate None Comedy None None NaN 6.8 865.0 3 Let's Talk About Sex 373615.0 373615.0 NaN 300000.0 Sep 11 1998 None NaN Fine Line None Comedy None None 13.0 NaN NaN 4 Slam 1009819.0 1087521.0 NaN 1000000.0 Oct 09 1998 R NaN Trimark Original Screenplay Drama Contemporary Fiction None 62.0 3.4 165.0 <p>Unas de las cosas m\u00e1s significativas de Jupyter notebook es poder trabajar con distintos tipos de gr\u00e1ficos (imagen est\u00e1tica o interactiva). Estos son de bastante utilidad para poder comprender nuestros procedimientos.</p> In\u00a0[4]: Copied! <pre>#collapse-hide\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n</pre> #collapse-hide  # select a point for which to provide details-on-demand label = alt.selection_single(     encodings=['x'], # limit selection to x-axis value     on='mouseover',  # select on mouseover events     nearest=True,    # select data point nearest the cursor     empty='none'     # empty selection includes no data points )  # define our base line chart of stock prices base = alt.Chart().mark_line().encode(     alt.X('date:T'),     alt.Y('price:Q', scale=alt.Scale(type='log')),     alt.Color('symbol:N') )  alt.layer(     base, # base line chart          # add a rule mark to serve as a guide line     alt.Chart().mark_rule(color='#aaa').encode(         x='date:T'     ).transform_filter(label),          # add circle marks for selected time points, hide unselected points     base.mark_circle().encode(         opacity=alt.condition(label, alt.value(1), alt.value(0))     ).add_selection(label),      # add white stroked text to provide a legible background for labels     base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(         text='price:Q'     ).transform_filter(label),      # add text labels for stock prices     base.mark_text(align='left', dx=5, dy=-5).encode(         text='price:Q'     ).transform_filter(label),          data=stocks ).properties(     width=500,     height=400 ) Out[4]: In\u00a0[5]: Copied! <pre>%lsmagic\n</pre> %lsmagic Out[5]: <pre>Available line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.</pre> <p>En varias situaciones resulta necesario medir el tiempo de ejecuci\u00f3n de una porci\u00f3n de c\u00f3digo. Para ello podemos usar la magia <code>%timeit</code>. Esta magia est\u00e1 disponible tanto para l\u00ednea como para celda:</p> In\u00a0[6]: Copied! <pre>%%timeit \n1+1 # timeit repite (adaptativamente) la medici\u00f3n a fin de reducir el error.\n</pre> %%timeit  1+1 # timeit repite (adaptativamente) la medici\u00f3n a fin de reducir el error. <pre>8.68 ns \u00b1 0.387 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000000 loops each)\n</pre> <p>Jupyter notebook permite tambi\u00e9n mezclar varios lenguajes de programaci\u00f3n en una misma notebook. Por ejemplo, podr\u00edamos escribir en bash lo siguiente:</p> In\u00a0[7]: Copied! <pre>%%bash\nfor i in {3..1}; do\n    echo $i\ndone\necho \"Hola desde $BASH\"\n</pre> %%bash for i in {3..1}; do     echo $i done echo \"Hola desde $BASH\" <pre>3\n2\n1\nHola desde /usr/bin/bash\n</pre> <p>Tambi\u00e9n, puede acceder a la l\u00ednea de comandos, anteponiendo el s\u00edmbolo de <code>!</code>. Esto es de bastante utilidad cuando se quiere mostrar las dependencias que se necesitan instalar. (ejemplo: <code>!pip install pandas</code>).</p> <p>Veamos un ejemplo:</p> In\u00a0[10]: Copied! <pre>!pwd\n</pre> !pwd <pre>/home/fralfaro/PycharmProjects/ds_blog/_notebooks\r\n</pre>"},{"location":"blog/2021/2021-07-31-jupyter/#jupyter-noteboook","title":"Jupyter Noteboook\u00b6","text":""},{"location":"blog/2021/2021-07-31-jupyter/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Jupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar c\u00f3digo en Python (por defecto, aunque permite otros lenguajes) de manera din\u00e1mica, a la vez que integrar en un mismo documento tanto bloques de c\u00f3digo como texto, gr\u00e1ficas o im\u00e1genes. Es un SaaS utilizado ampliamente en an\u00e1lisis num\u00e9rico, estad\u00edstica y machine learning, entre otros campos de la inform\u00e1tica y las matem\u00e1ticas.</p> <p>Por otro lado, JupyterLab es similar a Jupyter Notebook en cuanto a sus funcionalidade, pero tiene un interfaz m\u00e1s interesante para los usuarios. Eventualmente Jupyter Lab reemplazar\u00e1 a Jupyter Notebok.</p> <p>Nos centraremos en comprender aspectos b\u00e1sicos de c\u00f3mo trabajar un archivo en jupyter notebook (extensi\u00f3n <code>.ipynb</code>).</p>"},{"location":"blog/2021/2021-07-31-jupyter/#primeros-pasos","title":"Primeros Pasos\u00b6","text":""},{"location":"blog/2021/2021-07-31-jupyter/#instalacion","title":"Instalaci\u00f3n\u00b6","text":"<p>Para instalar RISE, necesitar\u00e1 usar la l\u00ednea de comando. Si ha instalado Anaconda, puede usar:</p> <pre><code>conda install -c conda-forge notebook\n</code></pre> <p>De lo contrario, puede instalar con pip:</p> <pre><code>pip install notebook\n</code></pre> <p>Nota: SI desea instalar JupyterLab, simplemente reemplaza <code>notebook</code> por <code>jupyterlab</code>.</p>"},{"location":"blog/2021/2021-07-31-jupyter/#primeros-pasos","title":"Primeros pasos\u00b6","text":""},{"location":"blog/2021/2021-07-31-jupyter/#notebook-server","title":"Notebook Server\u00b6","text":"<p>Una vez que haya instalado Jupyter Notebook en su computadora, estar\u00e1 listo para ejecutar el servidor de la computadora port\u00e1til. Puede iniciar el servidor del port\u00e1til desde la l\u00ednea de comandos (usando Terminal en Mac/Linux, S\u00edmbolo del sistema en Windows) ejecutando:</p> <pre><code>jupyter notebook\n\n</code></pre> <p>Esto imprimir\u00e1 cierta informaci\u00f3n sobre el servidor en su terminal, incluida la URL de la aplicaci\u00f3n web (de forma predeterminada, <code>http://localhost:8888</code>):</p> <pre><code>$ jupyter notebook\n[I 08:58:24.417 NotebookApp] Serving notebooks from local directory: /Users/catherine\n[I 08:58:24.417 NotebookApp] 0 active kernels\n[I 08:58:24.417 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n[I 08:58:24.417 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre>"},{"location":"blog/2021/2021-07-31-jupyter/#toolbox","title":"Toolbox\u00b6","text":"<p>Jupyter notebook nos ofrece el siguiente toolbox:</p> <p></p> <ul> <li><p>File: En \u00e9l, puede crear un nuevo cuaderno o abrir uno preexistente. Aqu\u00ed es tambi\u00e9n a donde ir\u00eda para cambiar el nombre de un Cuaderno. Creo que el elemento de men\u00fa m\u00e1s interesante es la opci\u00f3n Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita.</p> </li> <li><p>Edit: Aqu\u00ed puede cortar, copiar y pegar celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aqu\u00ed tambi\u00e9n.</p> </li> <li><p>View: es \u00fatil para alternar la visibilidad del encabezado y la barra de herramientas. Tambi\u00e9n puede activar o desactivar los n\u00fameros de l\u00ednea dentro de las celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quieres meterte con la barra de herramientas de la celda.</p> </li> <li><p>Insert: es solo para insertar celdas encima o debajo de la celda seleccionada actualmente.</p> </li> <li><p>Cell: le permite ejecutar una celda, un grupo de celdas o todas las celdas. Tambi\u00e9n puede ir aqu\u00ed para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es m\u00e1s intuitiva para eso.</p> </li> <li><p>Kernel: es para trabajar con el kernel que se ejecuta en segundo plano. Aqu\u00ed puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que est\u00e1 utilizando su computadora port\u00e1til.</p> </li> <li><p>Widgets: es para guardar y borrar el estado del widget. Los widgets son b\u00e1sicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido din\u00e1mico utilizando Python (u otro Kernel).</p> </li> <li><p>Help: es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia.</p> </li> </ul>"},{"location":"blog/2021/2021-07-31-jupyter/#markdown","title":"Markdown\u00b6","text":"<p>Jupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, t\u00edtulos de distintos tama\u00f1os, etc., de forma simple. Para ello Jupyter nos permite usar Markdown, que es un lenguaje de marcado (markup) muy popular.</p> <p>Los lenguajes de markup son lenguajes ideados para procesar texto, algunos de los m\u00e1s conocidos son HTML y $\\LaTeX$. Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles.</p> <p>La cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que m\u00e1s se utilizan.</p> <ul> <li><p>Texto en negrita/cursiva: El texto en negrita se indica entre dos pares de asteriscos. De este modo <code>**palabra**</code> aparecer\u00e1 como palabra. Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir <code>*palabra*</code> aparecer\u00e1 como palabra.</p> </li> <li><p>Listas: Las listas en Markdown se realizan indicando un asterisco o un n\u00famero seguido de un punto si se desean listas numeradas. Markdown organiza autom\u00e1ticamente los items asign\u00e1ndoles el n\u00famero correcto.</p> </li> <li><p>Inclusi\u00f3n de im\u00e1genes: La sintaxis para incluir im\u00e1genes en Markdown es <code>![nombre alternativo](direcci\u00f3n de la imagen)</code> en donde el nombre alternativo aparecer\u00e1 en caso de que no se pueda cargar la im\u00e1gen y la direcci\u00f3n puede referirse a una imagen local o un enlace en Internet.</p> </li> <li><p>Inclusi\u00f3n de c\u00f3digo HTML: El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el c\u00f3digo HTML.</p> </li> <li><p>Enlaces: Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a p\u00e1ginas en internet u otros archivos locales. Su sintaxis es <code>[texto](direcci\u00f3n del enlace)</code>.</p> </li> <li><p>F\u00f3rmulas matem\u00e1ticas: Gracias al uso de MathJax, se puede incluir c\u00f3digo en $\\LaTeX$ para mostrar todo tipo de f\u00f3rmulas y expresiones matem\u00e1ticas. Las f\u00f3rmulas dentro de una l\u00ednea de texto se escriben entre s\u00edmbolos de d\u00f3lar <code>$...$</code>, mientras que las expresiones separadas del texto utilizan s\u00edmbolos de d\u00f3lar dobles <code>$$...$$</code>. Los siguientes son ejemplos de f\u00f3rmulas matem\u00e1ticas escritas en $\\LaTeX$:</p> </li> </ul>"},{"location":"blog/2021/2021-07-31-jupyter/#codigo","title":"C\u00f3digo\u00b6","text":"<p>Jupyter Notebook permite que escribamos c\u00f3digo dependiendo del kernel a trabajar. Por defecto, se trabaja con el kernel de Python.</p> <p>Veamos unos ejemplos sencillos de c\u00f3digo:</p>"},{"location":"blog/2021/2021-07-31-jupyter/#completado-mediantes-tabs","title":"Completado mediantes Tabs.\u00b6","text":"<p>La completaci\u00f3n mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que est\u00e9 tratando.</p> <p>Simplemente escriba <code>object_name.&lt;TAB&gt;</code> para ver los atributos del objeto. Adem\u00e1s de los objetos y palabras clave de Python, la finalizaci\u00f3n de pesta\u00f1as tambi\u00e9n funciona en nombres de archivos y directorios.</p> <pre>import collections\ncollections. # aprete la tecla &lt;\ud835\udc47\ud835\udc34\ud835\udc35&gt;\n</pre>"},{"location":"blog/2021/2021-07-31-jupyter/#buscando-ayuda","title":"Buscando ayuda\u00b6","text":"<p>En caso de necesitar ayuda sobre cualquier comando de Python, Jupyter nos ofrece una funci\u00f3n llamada <code>help</code>.</p> <p>En resumen, \u00a1suele ser m\u00e1s importante saber como buscar informaci\u00f3n que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogaci\u00f3n <code>?</code> luego del nombre del comando (y luego ejecutar la celda con la combinaci\u00f3n de teclas SHIFT + ENTER).</p> <pre>import numpy as np\nnp.sum?\n</pre>"},{"location":"blog/2021/2021-07-31-jupyter/#magics","title":"Magics\u00b6","text":"<p>Jupyter posee varias funciones m\u00e1gicas predefinidas que sirven para simplificar tareas comunes.</p> <p>Hay dos tipos de magias:</p> <ul> <li><p>Magias por linea (line magics): son comandos que empiezan con el caracter <code>%</code> y que toman como argumentos valores escritos en la misma l\u00ednea.</p> </li> <li><p>Magias por celda (cell magics): son comandos que empiezan con los caracteres <code>%%</code>, y que reciben argumentos en la misma l\u00ednea y en toda la celda.</p> </li> </ul> <p>En general solo se puede usar una sola m\u00e1gias por celda en cada celda y debe ser escrita en la primer linea de la celda.</p> <p>Un buen ejemplo de m\u00e1gia es <code>%lsmagic</code> que lista todas las magias disponibles:</p>"},{"location":"blog/2021/2021-07-31-jupyter/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Notebook Basics</li> <li>Running the Notebook</li> </ul>"},{"location":"blog/2021/2021-08-05-rise/","title":"RISE","text":"<p>Algunas caracter\u00edsticas importantes del uso de RISE:</p> <ul> <li>Simplifica la generaci\u00f3n de material.</li> <li>Se mantiene un archivo y no varios archivos para hablar de lo mismo.</li> <li>Es f\u00e1cil de corregir, no se necesita mucho esfuerzo (similar a una PPT).</li> </ul> <p>En esta sesi\u00f3n, se muestra un ejemplo de c\u00f3mo crear una presentaci\u00f3n con RISE.</p> <p>Nota: Puede encontrar los c\u00f3digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaci\u00f3n con GitLab CI/CD.</p> <p>De lo contrario, puede instalar con pip:</p> <pre><code>pip install RISE\n</code></pre> <p>Nota: No interactuar\u00e1s directamente con RISE. En su lugar, podr\u00e1 acceder a \u00e9l a trav\u00e9s de Jupyter Notebooks.</p> <ul> <li>Haga clic en \"Ver\" en la barra de herramientas de Jupyter</li> <li>Coloca el cursor sobre \"Barra de herramientas de celda\" en el men\u00fa \"Ver\"</li> <li>Haga clic en \"Presentaci\u00f3n de diapositivas\" en el men\u00fa \"Barra de herramientas de celda\"</li> </ul> <p></p> <p>Deber\u00eda ver seis opciones aqu\u00ed. Este men\u00fa desplegable y sus opciones determinan c\u00f3mo encaja cada celda en la presentaci\u00f3n. Las opciones y sus descripciones se encuentran a continuaci\u00f3n:</p> <ul> <li>slide: indica que la celda seleccionada debe ser el comienzo de una nueva diapositiva.</li> <li>sub-slide -: indica que la celda seleccionada debe ser el comienzo de una nueva sub-diapositiva, que aparece en un nuevo marco debajo de la diapositiva anterior.</li> <li>fragment: indica que la celda seleccionada debe aparecer como una compilaci\u00f3n de la diapositiva anterior.</li> <li>skip: indica que la celda seleccionada debe omitirse y no ser parte de la presentaci\u00f3n de diapositivas.</li> <li>notes: indica que la celda seleccionada debe ser solo notas del presentador.</li> <li>- -: indica que la celda seleccionada debe seguir el comportamiento de la celda anterior, lo cual es \u00fatil cuando una celda de rebaja y una celda de c\u00f3digo deben aparecer simult\u00e1neamente.</li> </ul> <p>Cada una de estas opciones puede incluir c\u00f3digo Python o c\u00f3digo Markdown/HTML/LaTeX como un Jupyter Notebook tradicional.</p> <ul> <li>Usar el acceso directo OPTION + R shortcut (ALT + R on Windows) para ingresar y salir del modo de presentaci\u00f3n desde dentro de la computadora port\u00e1til</li> <li>Al hacer clic en el bot\u00f3n \"Modo de presentaci\u00f3n\" de la computadora port\u00e1til, esto solo aparecer\u00e1 si ha instalado RISE.</li> </ul> <p></p> <p>Despu\u00e9s de ingresar al modo de presentaci\u00f3n, deber\u00eda ver una pantalla similar a esta:</p> <p></p> <p>Hay muchos otros atajos de teclado a los que se puede acceder dentro de la presentaci\u00f3n haciendo clic en el signo de interrogaci\u00f3n (?) en la esquina inferior izquierda.</p> <p></p>"},{"location":"blog/2021/2021-08-05-rise/#rise","title":"RISE\u00b6","text":""},{"location":"blog/2021/2021-08-05-rise/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>RISE es una extensi\u00f3n a los jupyter notebooks que permite transformar tus notebooks en presentaciones interactivas.</p> <p>Toda las celdas pueden editarse y ejecutarse directamente, durante la presentaci\u00f3n. Esto es pr\u00e1ctico si necesitas corregir un error en una celda de texto. M\u00e1s importante a\u00fan, puedes ejecutar c\u00f3digo directamente en el kernel. En una misma diapositiva puedes tener m\u00faltiples celdas y elegir cu\u00e1l ejecutar, o corregir el texto y volver a ejecutar.</p> <p></p>"},{"location":"blog/2021/2021-08-05-rise/#primeros-pasos","title":"Primeros Pasos\u00b6","text":""},{"location":"blog/2021/2021-08-05-rise/#instalacion","title":"Instalaci\u00f3n\u00b6","text":"<p>Para instalar RISE, necesitar\u00e1 usar la l\u00ednea de comando. Si ha instalado Anaconda, puede usar:</p> <pre><code>conda install -c conda-forge rise\n</code></pre>"},{"location":"blog/2021/2021-08-05-rise/#habilitacion-del-modo-de-presentacion","title":"Habilitaci\u00f3n del modo de presentaci\u00f3n\u00b6","text":"<p>Para crear una presentaci\u00f3n, deber\u00e1 iniciar Jupyter Notebooks y abrir un nuevo notebook (tenga en cuenta que debe hacer esto despu\u00e9s de haber instalado RISE). Una vez que tenga un Jupyter Notebook nuevo, deber\u00e1 habilitar la presentaci\u00f3n de diapositivas. Puede hacer esto haciendo lo siguiente:</p>"},{"location":"blog/2021/2021-08-05-rise/#creando-las-diapositivas-con-celdas","title":"Creando las diapositivas con celdas\u00b6","text":"<p>En este punto, deber\u00eda tener una barra de herramientas de celda con un men\u00fa desplegable en el lado derecho: </p>"},{"location":"blog/2021/2021-08-05-rise/#ver-la-presentacion-de-diapositivas","title":"Ver la presentaci\u00f3n de diapositivas\u00b6","text":"<p>Una vez que se han utilizado las celdas para crear material para la presentaci\u00f3n, la presentaci\u00f3n se puede ver directamente desde el notebook.</p> <p>Hay dos opciones para ver la presentaci\u00f3n de diapositivas:</p>"},{"location":"blog/2021/2021-08-05-rise/#cambio-de-diapositivas","title":"Cambio de diapositivas\u00b6","text":"<p>Si bien puede ser tentador usar las teclas &lt;- y -&gt; para cambiar las diapositivas en la presentaci\u00f3n, esto no funcionar\u00e1 por completo: omitir\u00e1 las celdas marcadas como sub-slides. En su lugar, se debe usar ESPACIO para mover la presentaci\u00f3n de diapositivas hacia adelante y MAY\u00daS + ESPACIO para mover la presentaci\u00f3n de diapositivas hacia atr\u00e1s.</p>"},{"location":"blog/2021/2021-08-05-rise/#ejecucion-de-codigo-y-edicion-sobre-la-marcha","title":"Ejecuci\u00f3n de c\u00f3digo y edici\u00f3n sobre la marcha\u00b6","text":"<p>Una de las mejores cosas de RISE es que funciona en una sesi\u00f3n de Python en vivo, lo que significa que puede editar y ejecutar c\u00f3digo mientras se ejecuta la presentaci\u00f3n.</p>"},{"location":"blog/2021/2021-08-05-rise/#exportar-presentacion","title":"Exportar presentaci\u00f3n\u00b6","text":"<p>Puedes exportar tu presentaci\u00f3n desplegando la opci\u00f3n: <code>File -&gt; Download as</code>.</p> <p>Nota: Para poder descargar en formato <code>.pdf</code>, necesita tener instalado <code>pandoc</code>.</p>"},{"location":"blog/2021/2021-08-05-rise/#referencias","title":"Referencias\u00b6","text":"<ul> <li>RISE - Documentation</li> <li>Creating Interactive Slideshows in Jupyter Notebooks</li> </ul>"},{"location":"blog/2021/2021-08-11-jb/","title":"Jupyter Book","text":"<p>en donde:</p> <ul> <li><code>_config.yml</code>: archivo que contiene las configuraciones del proyecto.</li> <li><code>_toc.yml</code>: archivo que ordena los cap\u00edtulos del libro.</li> <li><code>content.md</code>: archivo gen\u00e9rico <code>.md</code>.</li> <li><code>intro.md</code>: archivo gen\u00e9rico <code>.md</code>.</li> <li><code>markdown.md</code>: archivo gen\u00e9rico <code>.md</code>.</li> <li><code>notebooks.ipynb</code>: archivo gen\u00e9rico <code>.ipynb</code>.</li> <li><code>references.bib</code>: archivo para a\u00f1adir las referencias.</li> <li><code>requirements.txt</code>: archivo que contiene las dependencias  python) del proyecto.</li> </ul> <p>Si bien la imagen puede estar contenida y referenciada desde el directorio ra\u00edz, tambi\u00e9n se puede incluir im\u00e1genes a trav\u00e9s de URL. Incluyamos una imagen del planeta J\u00fapiter en nuestro archivo <code>Introduction.md</code> usando lo siguiente:</p> <pre>\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter!\n```\n</pre> <p>La raz\u00f3n por la que le damos un \"nombre\" a nuestra imagen es para que podamos hacer referencia a ella f\u00e1cilmente con la sintaxis:</p> <pre><code>{numref}`jupiter-figure`\n</code></pre> <p>Se agregar\u00e1 una oraci\u00f3n que incluya esta referencia. El archivo completo ahora deber\u00eda verse as\u00ed:</p> <pre>\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n</pre> <p>En este punto, probablemente se deber\u00eda crear nuetro libro para asegurarnos de que tenga el aspecto esperado. Para hacer eso, primero necesitamos modificar nuestro archivo <code>_toc.yml</code>. Este archivo contiene la tabla de contenido de nuestro libro. Abra ese archivo ahora y elimine todo lo que hay all\u00ed. Luego, simplemente agregue lo siguiente:</p> <pre><code>- file: introduction\n</code></pre> <p>Ahora podemos construir nuestro libro desde la l\u00ednea de comandos asegur\u00e1ndonos de que estamos en el directorio ra\u00edz de nuestro libro y luego usando:</p> <pre><code>jupyter-book build .\n</code></pre> <p>Una vez finalizada la compilaci\u00f3n, tendr\u00e1 un nuevo subdirectorio llamado<code>_build/html/</code> en la ra\u00edz de su libro, navegue hasta esa ubicaci\u00f3n y abra <code>_build/html/index.html</code>. Deber\u00eda verse algo como esto:</p> <p></p> <pre>\nJupiter has a mass of:  $m_{j} \\approx 1.9 \\times 10^{27} kg$\n</pre> <p>Jupiter has a mass of:  $m_{j} \\approx 1.9 \\times 10^{27} kg$</p> <p>Los bloques matem\u00e1ticos se pueden definir usando la notaci\u00f3n <code>$$</code>: <pre>\n$$m_{j} \\approx 1.9 \\times 10^{27} kg$$\n</pre> $$m_{j} \\approx 1.9 \\times 10^{27} kg$$</p> <p>Nota: Si lo prefiere, los bloques matem\u00e1ticos tambi\u00e9n se pueden definir con <code>\\begin{equation}</code> en lugar de <code>$$</code>.</p> <p>Las ecuaciones numeradas se pueden definir as\u00ed (este es el estilo que te recomiendo que uses con m\u00e1s frecuencia):</p> <pre>\n```{math}\n:label: my_label\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n</pre> <p>Agreguemos m\u00e1s contenido a nuestro libro. Copie y agregue el siguiente texto a su archivo <code>Introduction.md</code>:</p> <pre>\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n```{math}\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```{math}\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n</pre> <p>A continuaci\u00f3n, puede reconstruir su libro (<code>jupyter-book build .</code>) y abrir <code>_build/html/index.html</code> para asegurarse de que todo se est\u00e9 procesando como se esperaba.</p> <p>No dude en agregar la siguiente advertencia a <code>Introduction.md</code>:</p> <pre>\n```{hint}\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n</pre> <p>Nota: Consulte la documentaci\u00f3n de BibTex para obtener informaci\u00f3n sobre el estilo de referencia de BibTex. Google Scholar facilita la exportaci\u00f3n de un formato de cita bibtex.</p> <p>A continuaci\u00f3n, puede hacer referencia al trabajo en su libro utilizando la siguiente directiva:</p> <pre><code>{cite}`mayor1995jupiter`\n</code></pre> <p>O para m\u00faltiples citas:</p> <pre><code>{cite}`mayor1995jupiter,guillot1999interiors`\n</code></pre> <p>Luego puede crear una bibliograf\u00eda a partir de <code>reference.bib</code> usando:</p> <pre>\n```{bibliography} references.bib\n```\n</pre> <p>Por ejemplo, intente agregar esto a su archivo <code>Introduction.md</code>:</p> <pre>\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```{bibliography} references.bib\n```\n</pre> <p>Su archivo final <code>Introduction.md</code> deber\u00eda verse as\u00ed:</p> <pre>\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n```{math}\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```{math}\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n```{margin} Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n```{hint}\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```{bibliography} references.bib\n```\n</pre> <p>Y deber\u00eda renderizarse as\u00ed: </p> <p>\u00a1Ahora intente construir su libro (<code>jupyter-book build .</code>) para asegurarse de que todo se vea bien! Usando la barra de contenido del lado izquierdo, navegue a la nueva p\u00e1gina \u201cThe Great Red Spot\u201d, que deber\u00eda verse as\u00ed:</p> <p></p> <p>\u00a1Ok genial! Ahora importemos los datos a los que hicimos referencia para que podamos crear algunos gr\u00e1ficos.</p> <p>Cree una nueva celda de c\u00f3digo debajo de la celda de rebaja actual y agregue el siguiente c\u00f3digo para leer en nuestro conjunto de datos de GRS como un marco de datos de Pandas.</p> <pre>import pandas as pd\npd.options.plotting.backend = \"plotly\"\n\nurl = \"https://raw.githubusercontent.com/UBC-DSCI/jupyterdays/master/jupyterdays/sessions/beuzen/data/GRS_data.csv\"\ndf = pd.read_csv(url)\ndf['Year'] = df['Year'].astype(int) \ndf.head()\n</pre> <p>Nota: Estamos imprimiendo la salida en la pantalla con el uso de <code>df.head()</code> y esto se mostrar\u00e1 en nuestro Jupyter Book renderizado.</p> <p>Si reconstruye su libro (<code>jupyter-book build .</code>) en este punto, ver\u00e1 algo como lo siguiente:</p> <p></p> <p>Ahora, podemos usar estos datos para crear algunos gr\u00e1ficos.</p> <p>Las tramas en su Jupyter Book pueden ser est\u00e1ticas (por ejemplo, <code>matplotlib</code>, <code>seaborn</code>) o interactivas (por ejemplo, <code>altair</code>, <code>plotly</code>, <code>bokeh</code>). Para este tutorial, crearemos algunos gr\u00e1ficos de ejemplo usando <code>Plotly</code> (a trav\u00e9s del backend de Pandas).</p> <p>Primero creemos un diagrama de dispersi\u00f3n simple de nuestros datos. Cree una nueva celda de c\u00f3digo en su cuaderno y agregue el siguiente c\u00f3digo:</p> <pre>import plotly.io as pio\npio.renderers.default = \"notebook\"\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\", color=\"Recorder\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=650, height=400)\nfig.update_layout(title={'text': \"Great Red Spot Size\", 'x':0.5, 'y':0.92})\nfig.update_traces(marker=dict(size=7))\n</pre> <p>Ya que estamos en eso, creemos tambi\u00e9n una trama animada. Cree otra celda de c\u00f3digo nueva y agregue el siguiente c\u00f3digo:</p> <pre>fig = df.plot.scatter(x=\"Year\", y=\"GRS Length\",\n                      animation_frame=\"Year\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=600, height=520)\nfig.update_layout(title={'text': \"Great Red Spot Size Animation\", 'x':0.5, 'y':0.94})\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 200\nfig.update_traces(marker=dict(size=10))\n</pre> <p>Nota: Plotly tiene diferentes renderizadores disponibles para generar gr\u00e1ficos. Es posible que deba experimentar con renderizadores para obtener el resultado que desea en su Jupyter Book. He descubierto que <code>pio.renderers.default = \"notebook\"</code> funciona con la versi\u00f3n actual de Jupyter Book.</p> <p>\u00a1Ahora, reconstruyamos nuestro libro y echemos un vistazo!</p> <p></p> <p>Es posible que desee ocultar parte del c\u00f3digo en su libro, \u00a1no hay problema! Eso tambi\u00e9n se hace f\u00e1cilmente con Jupyter Book.</p> <p>El que nos interesa aqu\u00ed es ocultar la entrada de c\u00f3digo. Podemos hacerlo f\u00e1cilmente agregando la etiqueta <code>hide-input</code> a la celda que deseamos ocultar. Hay varias formas de agregar etiquetas a la celda en Jupyter Notebooks. En Jupyter Lab, haga clic en el icono de engranaje en la barra lateral izquierda y luego agregue la etiqueta deseada como se muestra a continuaci\u00f3n:</p> <p></p> <p>Contin\u00fae y agregue las etiquetas <code>hide-input</code> a ambas celdas de trazado en su archivo <code>great_red_spot.ipynb</code>. Cuando reconstruyas el libro, ver\u00e1s que la entrada del c\u00f3digo est\u00e1 oculta (pero se puede alternar con el \u00edcono <code>+</code>):</p> <p></p> <p>Nota: Tambi\u00e9n puede almacenar el contenido de la libreta como valores, gr\u00e1ficos o marcos de datos en variables que se pueden utilizar en toda su libreta mediante la herramienta <code>glue</code>.</p>"},{"location":"blog/2021/2021-08-11-jb/#jupyter-book","title":"Jupyter Book\u00b6","text":""},{"location":"blog/2021/2021-08-11-jb/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Jupyter Book es un proyecto de c\u00f3digo abierto para crear libros y documentos mediante Jupyter Notebooks y/o Markdown.</p> <p>Algunas caracter\u00edsticas importantes del uso de Jupyter Book:</p> <ul> <li>contenido con calidad de publicaci\u00f3n que incluya figuras, s\u00edmbolos matem\u00e1ticos, citas y referencias cruzadas!</li> <li>escribir contenido como Jupyter Notebooks, markdown o reStructuredText</li> <li>Agregue interactividad a su libro, por ejemplo, alternar la visibilidad de las celdas, conectarse con un servicio en l\u00ednea como Binder e incluir resultados interactivos (por ejemplo, figuras y widgets).</li> <li>generar una variedad de resultados, incluidos sitios web (HTML, CSS, JS), markdown y PDF.</li> <li>una interfaz de l\u00ednea de comandos para crear libros r\u00e1pidamente, por ejemplo, <code>jupyter-book build mybook</code></li> </ul> <p>En esta sesi\u00f3n, se muestra un ejemplo de c\u00f3mo crear un Jupyter Book desde cero y algunas de las caracter\u00edsticas clave que ofrece Jupyter Book.</p> <p>Nota: Puede encontrar los c\u00f3digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaci\u00f3n con GitLab CI/CD.</p>"},{"location":"blog/2021/2021-08-11-jb/#primeros-pasos","title":"Primeros pasos\u00b6","text":""},{"location":"blog/2021/2021-08-11-jb/#instalacion","title":"Instalaci\u00f3n\u00b6","text":"<p>Para instalar Jupyter Book, necesitar\u00e1 usar la l\u00ednea de comando. Si ha instalado Anaconda, puede usar:</p> <pre><code>conda install -c conda-forge jupyter-book\n</code></pre> <p>De lo contrario, puede instalar con pip:</p> <pre><code>pip install jupyter-book\n</code></pre>"},{"location":"blog/2021/2021-08-11-jb/#crear-una-estructura-de-libro","title":"Crear una estructura de libro\u00b6","text":"<p>Jupyter Book viene con una herramienta que le permite crear y construir libros r\u00e1pidamente. Para crear el esqueleto del libro, escriba lo siguiente en la l\u00ednea de comando:</p> <pre><code>jupyter-book create jupiter\n</code></pre> <p>Nota: Aqu\u00ed llamamos al libro <code>jupiter</code>, pero puedes elegir llamar a tu libro como quieras.</p> <p>Ahora tendr\u00e1s un nuevo directorio llamado <code>jupiter</code> (o como quieras llamar a tu libro), con el siguiente contenido:</p> <pre><code>jupiter\n  \u251c\u2500\u2500 _config.yml\n  \u251c\u2500\u2500 _toc.yml\n  \u251c\u2500\u2500 content.md\n  \u251c\u2500\u2500 intro.md\n  \u251c\u2500\u2500 markdown.md\n  \u251c\u2500\u2500 notebooks.ipynb\n  \u251c\u2500\u2500 references.bib\n  \u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"blog/2021/2021-08-11-jb/#re-estructuracion-del-directorio","title":"Re-estructuraci\u00f3n del directorio\u00b6","text":"<p>Jupyter Book admite varios tipos de archivos:</p> <ul> <li>Markdown (.md)</li> <li>notebooks (.ipynb)</li> <li>etc.</li> </ul> <p>Como Markdown y Jupyter Notebooks probablemente ser\u00e1n los tipos de archivo m\u00e1s comunes que usar\u00e1, se mostrar\u00e1 un ejemplo de ello.</p> <p>Lo primero ser\u00e1 eliminar los archivos de inicio en el directorio:</p> <ul> <li>content.md</li> <li>intro.md</li> <li>markdown.md</li> <li>notebooks.ipynb</li> </ul> <p>As\u00ed que ejecutamos por l\u00ednea de comando:</p> <pre><code>rm content.md intro.md markdown.md notebooks.ipynb\n</code></pre> <p>Por otro lado, nuestro proyecto estar\u00e9 conformado por tres archivos:</p> <ul> <li>index.md</li> <li>Introduction.md</li> <li>great_red_spot.ipynb</li> </ul> <p>Luego, debemos indicar c\u00f3mo ser\u00e1n mostrados estos documentos en el archivo <code>_toc.yml</code>. La estructura ser\u00e1 la siguiente:</p> <pre><code>format: jb-book\nroot: index\nchapters:\n- file: Introduction\n- file: great_red_spot\n</code></pre> <p>En este caso, <code>root: index</code> corresponde al primer archivo que se visualiza en el jupyter-book. Dentro del archivo <code>index.md</code> escribiremos:</p> <pre>\n# Home\n\njupyter book example\n\n## Contenidos\n\n```{tableofcontents}\n```\n\n</pre>"},{"location":"blog/2021/2021-08-11-jb/#agregar-un-archivo-markdown","title":"Agregar un archivo Markdown\u00b6","text":"<p>Se comienza por agregar un archivo de markdown. Con alg\u00fan editor a elecci\u00f3n (por ejemplo, jupyter notebook o jupyterlab) se crea un nuevo archivo markdown llamado <code>Introduction.md</code>.</p> <p>Se usa este archivo como demostraci\u00f3n de algunos de los principales tipos de contenido que puede agregar en Jupyter-Book.</p>"},{"location":"blog/2021/2021-08-11-jb/#texto","title":"Texto\u00b6","text":"<p>Se agrega un texto de Markdown simple a nuestro archivo. Si no est\u00e1 familiarizado con la sintaxis de markdown, consulte Markdown Cheat Sheet. Puede copiar y pegar el siguiente contenido directamente en su archivo <code>Introduction.md</code>.</p> <pre><code># Jupiter Book\n\nThis book contains information about the planet **Jupiter** - the fifth planet from the sun and the largest planet in the solar system!\n</code></pre>"},{"location":"blog/2021/2021-08-11-jb/#figuras","title":"Figuras\u00b6","text":"<p>Puedes incluir figuras en tu Jupyter Book usando la siguiente sintaxis:</p> <pre>\n```{figure} my_image.png\n---\nheight: 150px\nname: my-image\n---\nHere is my image's caption!\n```\n</pre>"},{"location":"blog/2021/2021-08-11-jb/#equaciones-matematicas","title":"Equaciones matem\u00e1ticas\u00b6","text":"<p>Jupyter Book usa MathJax para componer matem\u00e1ticas, lo que le permite agregar matem\u00e1ticas de estilo LaTeX a su libro. Puede agregar matem\u00e1ticas en l\u00ednea, bloques matem\u00e1ticos y ecuaciones numeradas a su libro Jupyter. Sigamos adelante y creemos un nuevo encabezado en nuestro archivo Introduction.md que incluye algunas matem\u00e1ticas.</p> <p>Las matem\u00e1ticas en l\u00ednea se pueden definir usando <code>$</code> de la siguiente manera:</p>"},{"location":"blog/2021/2021-08-11-jb/#controlando-el-diseno-de-la-pagina","title":"Controlando el dise\u00f1o de la p\u00e1gina\u00b6","text":"<p>Hay varias formas diferentes de controlar el dise\u00f1o de las p\u00e1ginas de su Jupyter Book. El cambio de dise\u00f1o que utilizo con m\u00e1s frecuencia es agregar contenido a un margen en la p\u00e1gina. Puede agregar un margen usando la siguiente directiva:</p> <pre>\n```{margin} An optional title\nSome margin content.\n```\n</pre> <p>Agreguemos algo de contenido marginal al libro:</p> <pre>\n```{margin} Did you know?\nJupiter is 11.0x larger than Earth!\n```\n</pre>"},{"location":"blog/2021/2021-08-11-jb/#advertencias","title":"Advertencias\u00b6","text":"<p>Hay todo tipo de advertencias diferentes que puede usar en Jupyter Book que se enumeran aqu\u00ed en la documentaci\u00f3n de Jupyter Book. Las advertencias se crean con la sintaxis:</p> <pre>\n```{note}\nI am a useful note!\n```\n</pre>"},{"location":"blog/2021/2021-08-11-jb/#citas-y-bibliografia","title":"Citas y bibliograf\u00eda\u00b6","text":"<p>El \u00faltimo contenido corresponde a referencias y una bibliograf\u00eda. Puede agregar citas de cualquier trabajo almacenado en el archivo Bibtex <code>Reference.bib</code> que se encuentra en el directorio ra\u00edz de su libro.</p> <p>Para incluir una cita en su libro, agregue una entrada bibtex a <code>references.bib</code>, por ejemplo:</p> <pre><code>@article{mayor1995jupiter,\n    title={A Jupiter-mass companion to a solar-type star},\n    author={Mayor, Michel and Queloz, Didier},\n    journal={Nature},\n    volume={378},\n    number={6555},\n    pages={355--359},\n    year={1995},\n    publisher={Nature Publishing Group}\n}\n\n@article{guillot1999interiors,\n    title={Interiors of giant planets inside and outside the solar system},\n    author={Guillot, Tristan},\n    journal={Science},\n    volume={286},\n    number={5437},\n    pages={72--77},\n    year={1999},\n    publisher={American Association for the Advancement of Science}\n}\n</code></pre>"},{"location":"blog/2021/2021-08-11-jb/#agregar-un-archivo-de-contenido-de-jupyter-notebook","title":"Agregar un archivo de contenido de Jupyter Notebook\u00b6","text":"<p>Todos los flujos de trabajo de formato y estilo que vimos en markdown tambi\u00e9n se aplican a un Jupyter Notebook; simplemente agr\u00e9guelos a una celda de markdown y listo.</p> <p>Comencemos con lo siguiente:</p> <ul> <li>Cree un nuevo notebook llamado <code>great_red_spot.ipynb</code>;</li> <li>Agregue este archivo a su <code>_toc.yml</code>;</li> <li>Agregue una celda de markdown con el siguiente contenido:</li> </ul> <pre>\n# The Great Red Spot\n\nJupiter\u2019s iconic Great Red Spot (GRS) is actually an enormous storm that is bigger than Earth that has raged for hundreds of years! {numref}`great-red-spot` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/626_PIA21775.jpg\n---\nheight: 300px\nname: great-red-spot\n---\nJupiter's Great Red Spot! Source: [NASA](https://solarsystem.nasa.gov/resources/626/jupiters-great-red-spot-in-true-color/?category=planets_jupiter).\n```\n\nJupiter's GRS has been observed to be shrinking for about the last century and a half! [Here](https://github.com/UBC-DSCI/jupyterdays/tree/master/jupyterdays/sessions/beuzen/data) is some data of the length of the GRS spanning the last ~150 years which we can use to investigate this phenomenon.\n</pre>"},{"location":"blog/2021/2021-08-11-jb/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Jupyter-Book - Documentation</li> <li>Tutorial - Jupyter Book</li> </ul>"},{"location":"blog/2021/2021-08-31-buenas_practicas/","title":"Buenas Pr\u00e1cticas","text":"In\u00a0[1]: Copied! <pre>a = 10.  \nb = 3.5 \nprint(f\"El area es {a*b}\" )\n</pre> a = 10.   b = 3.5  print(f\"El area es {a*b}\" ) <pre>El area es 35.0\n</pre> <p>pero, \u00bfqu\u00e9 significan <code>a</code> y  <code>b</code>? lo sabemos por el comentario (bien hecho), pero si m\u00e1s adelante nos encontramos con esas variables, tendremos que recordar cual es cual. Es mejor usar nombres con significado:</p> In\u00a0[2]: Copied! <pre>altura = 10.  \nbase = 3.5 \nprint(f\"El area es {a*b}\" )\n</pre> altura = 10.   base = 3.5  print(f\"El area es {a*b}\" ) <pre>El area es 35.0\n</pre> In\u00a0[3]: Copied! <pre>print(\"Esta es una frase muy larga, se puede cortar con un \\\n       y seguir en la l\u00ednea inferior.\")\n</pre> print(\"Esta es una frase muy larga, se puede cortar con un \\        y seguir en la l\u00ednea inferior.\") <pre>Esta es una frase muy larga, se puede cortar con un        y seguir en la l\u00ednea inferior.\n</pre> In\u00a0[4]: Copied! <pre>#\n# esto es un comentario\nprint('Hola')\n</pre> # # esto es un comentario print('Hola') <pre>Hola\n</pre> <p>Tambi\u00e9n podemos tener comentarios multil\u00edneas:</p> In\u00a0[5]: Copied! <pre>#\n# Este es un comentario largo\n# y se extiende\n# a varias l\u00edneas\n</pre> # # Este es un comentario largo # y se extiende # a varias l\u00edneas In\u00a0[6]: Copied! <pre>#\n# no:\nimport sys, os\n</pre> # # no: import sys, os In\u00a0[7]: Copied! <pre>#\n# si:\nimport os\nimport sys\n</pre> # # si: import os import sys <pre># si\nif greeting:\n</pre> In\u00a0[8]: Copied! <pre>#\n# no\nlista_01 = [1, 2, 3,4, 5, 6,7, 8, 9,]\n</pre> # # no lista_01 = [1, 2, 3,4, 5, 6,7, 8, 9,] In\u00a0[9]: Copied! <pre>#\n# si \nlista_01 = [\n    1, 2, 3,\n    4, 5, 6,\n    7, 8, 9, \n]\n</pre> # # si  lista_01 = [     1, 2, 3,     4, 5, 6,     7, 8, 9,  ] <p>Aunque en Python se pueden hacer varias declaraciones en una l\u00ednea, se recomienda hacer s\u00f3lo una en cada l\u00ednea:</p> In\u00a0[10]: Copied! <pre>#\n# no\na = 10; b = 20\n</pre> # # no a = 10; b = 20 In\u00a0[11]: Copied! <pre>#\n# si\na = 10\nb = 20\n</pre> # # si a = 10 b = 20   <p>Cuando se trabaja con lista, conjuntos y/o tuplas se recomienda poner en cada l\u00ednea sus argumentos.</p> In\u00a0[12]: Copied! <pre>#\n# no\nlista = [(1, 'hola'),(2, 'mundo'),]\n</pre> # # no lista = [(1, 'hola'),(2, 'mundo'),]   In\u00a0[13]: Copied! <pre>#\n# si\nlista = [\n    (1, 'hola'),\n    (2, 'mundo'),\n]\n</pre> # # si lista = [     (1, 'hola'),     (2, 'mundo'), ] <p>Lo anterior se puede extender para funciones con muchos argumentos</p> In\u00a0[14]: Copied! <pre>#\n# no\ndef funcion_01(x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n    x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n</pre> # # no def funcion_01(x1,x2,x3,x4):     print(x1,x2,x3,x4)      def funcion_02(     x1,x2,x3,x4):     print(x1,x2,x3,x4) In\u00a0[15]: Copied! <pre>#\n# si\ndef funcion_01(x1,x2,\n               x3,x4):\n    \n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n        x1,x2,\n        x3,x4):\n    \n    print(x1,x2,x3,x4)\n</pre> # # si def funcion_01(x1,x2,                x3,x4):          print(x1,x2,x3,x4)      def funcion_02(         x1,x2,         x3,x4):          print(x1,x2,x3,x4) <pre># no\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n</pre> <pre># si\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n</pre> In\u00a0[16]: Copied! <pre>#\n# Seleccionar los n\u00fameros positivos\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = []\nfor i in numeros:\n    if i &gt; 0:\n        positivos.append(i)\n        \nprint(f\"positivos: {positivos}\")\n</pre> # # Seleccionar los n\u00fameros positivos numeros = [-3, 2, 1, -8, -2, 7] positivos = [] for i in numeros:     if i &gt; 0:         positivos.append(i)          print(f\"positivos: {positivos}\") <pre>positivos: [2, 1, 7]\n</pre> <p>Aunque t\u00e9cnicamente es correcto, es m\u00e1s eficiente hacer List Comprehension:</p> In\u00a0[17]: Copied! <pre>#\n# comprension de lista\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = [i for i in numeros if i &gt; 0] # List Comprehension\nprint(f\"positivos: {positivos}\")\n</pre> # # comprension de lista numeros = [-3, 2, 1, -8, -2, 7] positivos = [i for i in numeros if i &gt; 0] # List Comprehension print(f\"positivos: {positivos}\") <pre>positivos: [2, 1, 7]\n</pre> In\u00a0[18]: Copied! <pre>#\n# importar librerias\nimport sys\n</pre> # # importar librerias import sys In\u00a0[19]: Copied! <pre>#\n# no\ntry:\n    r = 1/0\nexcept:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n</pre> # # no try:     r = 1/0 except:     print(\"Oops! ocurrio un\",sys.exc_info()[0]) <pre>Oops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n</pre> In\u00a0[20]: Copied! <pre>#\n# si\ntry:\n    r = 1/0\nexcept ZeroDivisionError:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n</pre> # # si try:     r = 1/0 except ZeroDivisionError:     print(\"Oops! ocurrio un\",sys.exc_info()[0]) <pre>Oops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n</pre> In\u00a0[21]: Copied! <pre>#\n# no\nvalor = 5\n\ndef funcion_01(variable):\n    return 2*variable + valor\n</pre> # # no valor = 5  def funcion_01(variable):     return 2*variable + valor In\u00a0[22]: Copied! <pre>funcion_01(2)\n</pre> funcion_01(2) Out[22]: <pre>9</pre> In\u00a0[23]: Copied! <pre>#\n# si\ndef funcion_01(variable,valor):\n    return 2*variable + valor\n</pre> # # si def funcion_01(variable,valor):     return 2*variable + valor In\u00a0[24]: Copied! <pre>funcion_01(2,5)\n</pre> funcion_01(2,5) Out[24]: <pre>9</pre> In\u00a0[25]: Copied! <pre>#\n# escritura din\u00e1mica\ndef suma(x,y):\n    return x+y\n</pre> # # escritura din\u00e1mica def suma(x,y):     return x+y In\u00a0[26]: Copied! <pre>print(suma(1,2))\n</pre> print(suma(1,2)) <pre>3\n</pre> In\u00a0[27]: Copied! <pre>#\n# escritura estatica\ndef suma(x:float,\n         y:float)-&gt;float:\n    return x+y\n</pre> # # escritura estatica def suma(x:float,          y:float)-&gt;float:     return x+y In\u00a0[28]: Copied! <pre>print(suma(1,2))\n</pre> print(suma(1,2)) <pre>3\n</pre> <p>Para la escritura est\u00e1tica, si bien se especifica el tipo de atributo (tanto de los inputs o outputs), la funci\u00f3n puede recibir otros tipos de atributos.</p> In\u00a0[29]: Copied! <pre>print(suma(\"hola\",\" mundo\"))\n</pre> print(suma(\"hola\",\" mundo\")) <pre>hola mundo\n</pre> <p>Para validar los tipos de datos son los correctos, se deben ocupar librer\u00edas especializadas en la validaci\u00f3n de datos (por ejemplo: pydantic).</p> In\u00a0[30]: Copied! <pre>def potencia(x, y):\n\"\"\"\n    Calcula la potencia arbitraria de un numero\n    \"\"\"\n    return x**y\n</pre> def potencia(x, y):     \"\"\"     Calcula la potencia arbitraria de un numero     \"\"\"     return x**y In\u00a0[31]: Copied! <pre># Acceso a la documentaci\u00f3n\npotencia.__doc__\n</pre> # Acceso a la documentaci\u00f3n potencia.__doc__ Out[31]: <pre>'\\n    Calcula la potencia arbitraria de un numero\\n    '</pre> In\u00a0[32]: Copied! <pre># Acceso a la documentaci\u00f3n\nhelp(potencia)\n</pre> # Acceso a la documentaci\u00f3n help(potencia) <pre>Help on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n\n</pre> <p>Lo correcto es detallar lo mejor posible en el Docstring qu\u00e9 hace y c\u00f3mo se usa la funci\u00f3n o clase y los par\u00e1metros que necesita. Se recomienda usar el estilo de documentaci\u00f3n del software de documentaci\u00f3n sphinx, que emplea reStructuredText como lenguaje de marcado.</p> <p>Veamos un ejemplo de una funci\u00f3n bien documentada:</p> In\u00a0[33]: Copied! <pre>def potencia(x, y):\n\"\"\"\n    Calcula la potencia arbitraria de un numero\n\n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n    \"\"\"\n\n    return x**y\n</pre> def potencia(x, y):     \"\"\"     Calcula la potencia arbitraria de un numero      :param x: base     :param y: exponente     :return:  potencia de un numero     :ejemplos:          &gt;&gt;&gt; potencia(2, 1)     2     &gt;&gt;&gt; potencia(3, 2)     9     \"\"\"      return x**y In\u00a0[34]: Copied! <pre># Acceso a la documentaci\u00f3n\npotencia.__doc__\n</pre> # Acceso a la documentaci\u00f3n potencia.__doc__ Out[34]: <pre>'\\n    Calcula la potencia arbitraria de un numero\\n\\n    :param x: base\\n    :param y: exponente\\n    :return:  potencia de un numero\\n    :ejemplos:\\n    \\n    &gt;&gt;&gt; potencia(2, 1)\\n    2\\n    &gt;&gt;&gt; potencia(3, 2)\\n    9\\n    '</pre> In\u00a0[35]: Copied! <pre># Acceso a la documentaci\u00f3n\nhelp(potencia)\n</pre> # Acceso a la documentaci\u00f3n help(potencia) <pre>Help on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n    \n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n\n</pre> <p>Existen varias formas de documentar tus funciones, las principales encontradas en la literatura son:</p> <ul> <li>Google docstrings: forma de documentaci\u00f3n recomendada por Google..</li> <li>reStructured Text: est\u00e1ndar oficial de documentaci\u00f3n de Python; No es apto para principiantes, pero tiene muchas funciones.</li> <li>NumPy/SciPy docstrings: combinaci\u00f3n de NumPy de reStructured y Google Docstrings.</li> </ul> In\u00a0[1]: Copied! <pre>import this\n</pre> import this <pre>The Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n</pre> <p>Se destacan tres tipos de patrones de dise\u00f1os:</p> <ul> <li>Comportamiento</li> <li>Creacionales</li> <li>Estructurales</li> </ul> <p>En el siguiente link se deja una gu\u00eda para poder entender estos conceptos en python.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#buenas-practicas","title":"Buenas Pr\u00e1cticas\u00b6","text":""},{"location":"blog/2021/2021-08-31-buenas_practicas/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Una pregunta que surgue a menudo cuando uno se encuentra programando es saber cu\u00e1l es la forma correcta de programar. La respuesta es que no existe la forma correcta de programar (ya sea en Python o cualquier otro lenguaje), sin embargo, existen estandares dentro del mundo de la programaci\u00f3n, con el fin de hacer el c\u00f3digo m\u00e1s legible, sencillo de entender y ayudar a encontrar posibles errores.</p> <p>En esta secci\u00f3n se mostrar\u00e1 algunos conceptos sencillos que te ayudar\u00e1n a mejorar tus skills en el desarrollo de software (con Python).</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#estilo-de-codificacion-pep8","title":"Estilo de codificaci\u00f3n: PEP8\u00b6","text":"<p>El PEP8 es un estilo de codificaci\u00f3n que proporciona convenciones de codificaci\u00f3n para el c\u00f3digo Python que comprende la biblioteca est\u00e1ndar en la distribuci\u00f3n principal de Python.</p> <p>Algunos aspectos importantes:</p> <ul> <li><p>El PEP8 y el PEP 257 (Docstring Conventions) fueron adaptados del ensayo original de la Gu\u00eda de estilo Python de Guido, con algunas adiciones de la gu\u00eda de estilo de Barry.</p> </li> <li><p>Esta gu\u00eda de estilo evoluciona con el tiempo a medida que se identifican convenciones adicionales y las convenciones pasadas se vuelven obsoletas debido a cambios en el propio lenguaje.</p> </li> <li><p>Muchos proyectos tienen sus propias pautas de estilo de codificaci\u00f3n. En caso de conflicto, dichas gu\u00edas espec\u00edficas del proyecto tienen prioridad para ese proyecto.</p> </li> </ul> <p>Basados en el PEP8 y algunas buenas pr\u00e1cticas del dise\u00f1o de software, veamos ejemplo para poder escribir de mejor forma nuestros c\u00f3digos.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#variables","title":"Variables\u00b6","text":"<p>Cuando sea posible, define variables con nombres que tengan alg\u00fan sentido o que puedas identificar f\u00e1cilmente, no importa que sean m\u00e1s largas. Por ejemplo, en un programa podr\u00edamos escribir:</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#lineas-de-codigos","title":"Lineas de c\u00f3digos\u00b6","text":"<p>Las l\u00edneas de codigo no deben ser muy largas, como mucho 72 caracteres. Si se tiene una l\u00ednea larga, se puede cortar con una barra invertida (<code>\\</code>) y continuar en la siguiente l\u00ednea:</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#comentarios","title":"Comentarios\u00b6","text":"<p>Los comentarios son muy importantes al escribir un programa. Describen lo que est\u00e1 sucediendo dentro de un programa, para que una persona que mira el c\u00f3digo fuente no tenga dificultades para descifrarlo.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#importaciones","title":"Importaciones\u00b6","text":"<p>Las importaciones generalmente deben estar en l\u00edneas separadas:</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#comparaciones","title":"Comparaciones\u00b6","text":"<p>Existen varias formas de hacer comparaciones de objetos (principalmente en el uso del <code>bucle if</code>), ac\u00e1 se dejan alguna recomendaciones:</p> <pre># no\nif greeting == True:\n\n# no\nif greeting is True:\n</pre>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#identacion","title":"Identaci\u00f3n\u00b6","text":"<p>Dentro de par\u00e9ntesis, corchetes o llaves, no dejar espacios inmediatamente dentro de ellos:</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#operadores-binarios","title":"Operadores binarios\u00b6","text":"<p>Un tema interesante es corresponde a la identaci\u00f3n respecto a los operadores binarios, ac\u00e1 se muestra la forma correcta de hacerlo:</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#manipulacion-de-listas","title":"Manipulaci\u00f3n de listas\u00b6","text":"<p>Aunque combinar iterables con elementos de control de flujo para manipular listas es muy sencillo con Python, hay m\u00e9todos espec\u00edficos m\u00e1s eficientes para hacer lo mismo. Pensemos el fitrado de datos de una lista:</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#especificar-tipo-de-error","title":"Especificar tipo de error\u00b6","text":"<p>Cuando se ocupa <code>try/except</code>, es necesario especificar el tipo de error que se est\u00e1 cometiendo.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#explicitar-dependencias-de-una-funcion","title":"Explicitar dependencias de una funci\u00f3n\u00b6","text":"<p>Siempre es mejor definir las variables dentro de una funci\u00f3n y no dejar variables globales.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#dynamicstatic-typing","title":"Dynamic/Static typing\u00b6","text":"<p>Con Python 3 se puede especificar el tipo de par\u00e1metro y el tipo de retorno de una funci\u00f3n (usando la notaci\u00f3n PEP484 y PEP526. Se definen dos conceptos claves:</p> <ul> <li>Escritura  din\u00e1mica: no se especifican los atributos de los inputs ni de los ouputs</li> <li>Escritura  est\u00e1tica: se especifican los atributos de los inputs y los ouputs</li> </ul>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#librerias","title":"Librer\u00edas\u00b6","text":"<p>Existen librer\u00edas que pueden ayudar a corregir errores de escrituras en t\u00fa c\u00f3digo (tambi\u00e9n conocido como An\u00e1lisis Est\u00e1tico), por ejemplo:</p> <ul> <li>black: El formateador de c\u00f3digo inflexible.</li> <li>flake8: La herramienta para aplicar la gu\u00eda de estilo PEP8.</li> <li>mypy:  Mypy es un verificador de tipo est\u00e1tico para Python 3.</li> </ul>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#documentacion","title":"Documentaci\u00f3n\u00b6","text":"<p>Casi tan importante como la escritura de c\u00f3digo, es su correcta documentaci\u00f3n, una parte fundamental de cualquier programa que a menudo se infravalora o simplemente se ignora. Aparte de los comentarios entre el c\u00f3digo explicando c\u00f3mo funciona, el elemento b\u00e1sico de documentaci\u00f3n de Python es el Docstring o cadena de documentaci\u00f3n, que ya hemos visto. Simplemente es una cadena de texto con triple comillas que se coloca justo despu\u00e9s de la definici\u00f3n de funci\u00f3n o clase que sirve de documentaci\u00f3n a ese elemento.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#zen-de-python","title":"Zen de python\u00b6","text":"<p>El Zen de Python te dar\u00e1 la gu\u00eda para decidir sobre que hacer con tu c\u00f3digo, no te dice como lo debes escribir, sino como debes pensar si estas programando en Python.</p> <p>Principios importantes:</p> <ul> <li>Expl\u00edcito es mejor que impl\u00edcito: Que no se asuma nada, aseg\u00farate que las cosas sean.</li> <li>Simple es mejor que complejo: Evita c\u00f3digo complejo, c\u00f3digo espagueti o que hace mas cosas para poder hacer una simple tarea.</li> <li>Plano es mejor que anidado: Si tu c\u00f3digo tiene mas de 3 niveles de identaci\u00f3n, deber\u00edas mover parte de ese c\u00f3digo a una funci\u00f3n.</li> <li>Los errores nunca deber\u00edan pasar silenciosamente: No uses un Try/Except sin definir que tipo de error vas a cachar, viene de la mano con Explicito es mejor que impl\u00edcito.</li> <li>Si la implementaci\u00f3n es dif\u00edcil de explicar, es mala idea.</li> </ul> <p>Tambi\u00e9n, podemos ver el mensaje original del zen de python, ejecutando la siguiente linea de comando.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#mas-consejos","title":"M\u00e1s consejos\u00b6","text":"<p>Los consejos que se presentan son de mucha utilidad si usted quiere llevar sus conociminetos de programaci\u00f3n al siguiente nivel, sin embargo, el contenido de cada uno amerita un curso por si solo. Se deja recomienda al lector seguir profundizando en estos temas.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#entender-programacion-multiparadigma","title":"Entender programaci\u00f3n multiparadigma\u00b6","text":"<p>Python al ser multiparadigma, nos da una amplia gama de posibilidades de dise\u00f1ar nuestros c\u00f3digos. Dentro de estos se destacan:</p> <ul> <li>Programaci\u00f3n orientada a objetos (OOP)</li> <li>Programaci\u00f3n funcional</li> </ul> <p>Cu\u00e1ndo ocupar uno o la otra, va a depender de c\u00f3mo queremos abordar una determinada problem\u00e1tica, puesto que en la mayor\u00eda de los casos, se puede pasar de un paradigma a o otro (incluso mezclarlos de ser necesario).</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#principio-solid","title":"Principio S.O.L.I.D\u00b6","text":"<p>En ingenier\u00eda de software, SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) es un acr\u00f3nimo mnem\u00f3nico introducido por Robert C. Martin a comienzos de la d\u00e9cada del 2000 que representa cinco principios b\u00e1sicos de la programaci\u00f3n orientada a objetos y el dise\u00f1o. Cuando estos principios se aplican en conjunto es m\u00e1s probable que un desarrollador cree un sistema que sea f\u00e1cil de mantener y ampliar con el tiempo.</p> <p>En el siguiente link se deja una gu\u00eda para poder entender estos conceptos en python.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#patrones-de-diseno","title":"Patrones de dise\u00f1o\u00b6","text":"<p>Los patrones de dise\u00f1o son la base para la b\u00fasqueda de soluciones a problemas comunes en el desarrollo de software y otros \u00e1mbitos referentes al dise\u00f1o de interacci\u00f3n o interfaces.</p> <p>Un patr\u00f3n de dise\u00f1o es una soluci\u00f3n a un problema de dise\u00f1o.</p>"},{"location":"blog/2021/2021-08-31-buenas_practicas/#referencias","title":"Referencias\u00b6","text":"<ul> <li>The Clean Coder: A Code Of Conduct For Professional Programmers Robert C. Martin (2011)</li> <li>Clean Code: A Handbook of Agile Software - Robert C. Martin (2009).</li> <li>Working effectively with legacy code Michael C. Feathers (2004)</li> <li>Refactoring Martin Fowler (1999)</li> <li>The Pragmatic Programmer Thomas Hunt (1999)</li> </ul>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/","title":"Impact on Digital Learning","text":"In\u00a0[1]: hide-input Copied! <pre># libraries\nimport glob\nimport re\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# read data\n\n## products dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/'\nproducts_df = pd.read_csv(path + \"products_info.csv\")\nproducts_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]\n\n## districts dataset\ndistricts_df = pd.read_csv(path +\"districts_info.csv\")\n#districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')\n\n## engagement dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/' \nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split(\"/\")[-1].split(\".\")[0]\n    df[\"district_id\"] = district_id\n    li.append(df)\nengagement_df = pd.concat(li)\nengagement_df = engagement_df.reset_index(drop=True)\n\n# summary\n\ndf_list = [\n    districts_df,\n    products_df,\n    engagement_df\n]\n\ndf_name = [\n    'districts_df',\n    'products_df',\n    'engagement_df'\n]\n\ncols = [\n    'dataframe',\n    'column', \n    'dtype', \n    'Non-Null Count', \n    'Null Count',\n    'unique'\n    \n]\n\nframes=[]\n\n\nfor i in range(len(df_list)):\n    df = df_list[i].copy()\n    a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})\n    b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})\n    c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})\n    temp = a.merge(b,on = 'column').merge(c,on = 'column')\n    \n    dct = {col: len(df[col].unique()) for col in df.columns}\n    df_unique = pd.DataFrame({\n    'column':dct.keys(),\n    'unique':dct.values(),\n    })\n    temp = temp.merge(df_unique,on = 'column')\n    temp['dataframe'] = df_name[i]\n    frames.append(temp)\n</pre> # libraries import glob import re   import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots  from wordcloud import WordCloud, STOPWORDS  import warnings warnings.filterwarnings('ignore')   # read data  ## products dataset path = '../input/learnplatform-covid19-impact-on-digital-learning/' products_df = pd.read_csv(path + \"products_info.csv\") products_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]  ## districts dataset districts_df = pd.read_csv(path +\"districts_info.csv\") #districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')  ## engagement dataset path = '../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/'  all_files = glob.glob(path + \"/*.csv\")  li = []  for filename in all_files:     df = pd.read_csv(filename, index_col=None, header=0)     district_id = filename.split(\"/\")[-1].split(\".\")[0]     df[\"district_id\"] = district_id     li.append(df) engagement_df = pd.concat(li) engagement_df = engagement_df.reset_index(drop=True)  # summary  df_list = [     districts_df,     products_df,     engagement_df ]  df_name = [     'districts_df',     'products_df',     'engagement_df' ]  cols = [     'dataframe',     'column',      'dtype',      'Non-Null Count',      'Null Count',     'unique'      ]  frames=[]   for i in range(len(df_list)):     df = df_list[i].copy()     a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})     b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})     c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})     temp = a.merge(b,on = 'column').merge(c,on = 'column')          dct = {col: len(df[col].unique()) for col in df.columns}     df_unique = pd.DataFrame({     'column':dct.keys(),     'unique':dct.values(),     })     temp = temp.merge(df_unique,on = 'column')     temp['dataframe'] = df_name[i]     frames.append(temp) <ul> <li>Summary:</li> </ul> <ul> <li>Summary:</li> </ul> <ul> <li>Summary:</li> </ul> In\u00a0[2]: hide-input Copied! <pre># products_df\n\nproducts_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources &amp; References' : 'Sites, Resources &amp; Reference'})\n#products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\ntemp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_df = products_df.join(temp_sectors)\n#products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n#del temp_sectors\n\n# engagement_df\n\nengagement_df['time'] = pd.to_datetime(engagement_df['time'])\n\ntemp = engagement_df[['time']].drop_duplicates('time')\ntemp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1])\nengagement_df = engagement_df.merge(temp,on ='time')\n\nengagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int)\nengagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)\n\nengagement_df_mix = engagement_df.merge(\n    districts_df[['district_id','state']],\n    on = 'district_id'\n)\nengagement_df_mix = engagement_df_mix.merge(\n    products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],\n    on = 'lp_id'\n)\n</pre> # products_df  products_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x) products_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)  # Synchronize similar values products_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources &amp; References' : 'Sites, Resources &amp; Reference'}) #products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)  temp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \") temp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns] products_df = products_df.join(temp_sectors) #products_df.drop(\"Sector(s)\", axis=1, inplace=True)  #del temp_sectors  # engagement_df  engagement_df['time'] = pd.to_datetime(engagement_df['time'])  temp = engagement_df[['time']].drop_duplicates('time') temp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1]) engagement_df = engagement_df.merge(temp,on ='time')  engagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int) engagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)  engagement_df_mix = engagement_df.merge(     districts_df[['district_id','state']],     on = 'district_id' ) engagement_df_mix = engagement_df_mix.merge(     products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],     on = 'lp_id' ) In\u00a0[3]: hide-input Copied! <pre># map plot: districts\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\ntemp = pd.DataFrame({\n    'state_abbrev':us_state_abbrev.values(),\n})\n\ntemp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0)\ntemp['num_districts'] = temp['num_districts'].astype(int)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    title_font_color=\"black\",\n    geo_scope='usa',\n)    \n\n\nfig.add_trace(\n    go.Choropleth(\n        locations=temp.state_abbrev,\n        zmax=1,\n        z = temp.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='black',\n        geo='geo',\n        colorscale=px.colors.sequential.Greys, \n        \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()\n</pre> # map plot: districts  us_state_abbrev = {     'Alabama': 'AL',     'Alaska': 'AK',     'American Samoa': 'AS',     'Arizona': 'AZ',     'Arkansas': 'AR',     'California': 'CA',     'Colorado': 'CO',     'Connecticut': 'CT',     'Delaware': 'DE',     'District Of Columbia': 'DC',     'Florida': 'FL',     'Georgia': 'GA',     'Guam': 'GU',     'Hawaii': 'HI',     'Idaho': 'ID',     'Illinois': 'IL',     'Indiana': 'IN',     'Iowa': 'IA',     'Kansas': 'KS',     'Kentucky': 'KY',     'Louisiana': 'LA',     'Maine': 'ME',     'Maryland': 'MD',     'Massachusetts': 'MA',     'Michigan': 'MI',     'Minnesota': 'MN',     'Mississippi': 'MS',     'Missouri': 'MO',     'Montana': 'MT',     'Nebraska': 'NE',     'Nevada': 'NV',     'New Hampshire': 'NH',     'New Jersey': 'NJ',     'New Mexico': 'NM',     'New York': 'NY',     'North Carolina': 'NC',     'North Dakota': 'ND',     'Northern Mariana Islands':'MP',     'Ohio': 'OH',     'Oklahoma': 'OK',     'Oregon': 'OR',     'Pennsylvania': 'PA',     'Puerto Rico': 'PR',     'Rhode Island': 'RI',     'South Carolina': 'SC',     'South Dakota': 'SD',     'Tennessee': 'TN',     'Texas': 'TX',     'Utah': 'UT',     'Vermont': 'VT',     'Virgin Islands': 'VI',     'Virginia': 'VA',     'Washington': 'WA',     'West Virginia': 'WV',     'Wisconsin': 'WI',     'Wyoming': 'WY' }  districts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev) districts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False) districts_info_by_state.columns = ['state_abbrev', 'num_districts']  temp = pd.DataFrame({     'state_abbrev':us_state_abbrev.values(), })  temp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0) temp['num_districts'] = temp['num_districts'].astype(int)  fig = go.Figure() layout = dict(     title_text = \"Number of Available School Districts per State\",     title_font_color=\"black\",     geo_scope='usa', )       fig.add_trace(     go.Choropleth(         locations=temp.state_abbrev,         zmax=1,         z = temp.num_districts,         locationmode = 'USA-states', # set of locations match entries in `locations`         marker_line_color='black',         geo='geo',         colorscale=px.colors.sequential.Greys,               ) )              fig.update_layout(layout)    fig.show() In\u00a0[4]: hide-input Copied! <pre># bar plot: districts\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.countplot(\n    y=\"state\",\n    data=districts_df,\n    order=districts_df.state.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=16)\n\n#Text\nplotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nplotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85)\nplotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14)\nplotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\n\n\nplt.show()\n</pre> # bar plot: districts  plt.style.use('default') plt.figure(figsize=(14,8))  plotting = sns.countplot(     y=\"state\",     data=districts_df,     order=districts_df.state.value_counts().index,     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=16)  #Text plotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90); plotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85) plotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14) plotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)  plt.yticks(fontsize=14) plt.xticks(fontsize=14)    plt.show() In\u00a0[5]: hide-input Copied! <pre># heatmap: districts -&gt; locale\n\ntemp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\ntemp1 = districts_df.groupby('locale')['pct_black/hispanic'].value_counts().to_frame()\ntemp1.columns = ['amount']\n\ntemp1 = temp1.reset_index(drop=False)\ntemp1 = temp1.pivot(index='locale', columns='pct_black/hispanic')['amount']\n\ntemp2 = districts_df.groupby('locale')['pct_free/reduced'].value_counts().to_frame()\ntemp2.columns = ['amount']\n\ntemp2 = temp2.reset_index(drop=False)\n\ntemp2 = temp2.pivot(index='locale', columns='pct_free/reduced')['amount']\n\nplt.style.use('default')\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))\n\nsns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d')\nax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nax1.xaxis.set_tick_params(labelsize=16)\n\nfor container in ax1.containers:\n    ax1.bar_label(container,fontsize=16)\n\nsns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14})\nax2.set_title('Heatmap: locale and pct_black/hispanic',fontsize=16,loc='left')\nax2.xaxis.set_tick_params(labelsize=16)\nax2.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14})\nax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left')\nax3.xaxis.set_tick_params(labelsize=16)\nax3.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14})\nax4.set_title('Heatmap: locale and pct_free/reduced',fontsize=16,loc='left')\nax4.xaxis.set_tick_params(labelsize=16)\nax4.yaxis.set_tick_params(labelsize=16)\n\n\nplt.show()\n</pre> # heatmap: districts -&gt; locale  temp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame() temp.columns = ['amount']  temp = temp.reset_index(drop=False)  temp = temp.pivot(index='locale', columns='pp_total_raw')['amount'] temp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',        '[12000, 14000[', '[14000, 16000[', '[16000, 18000[',         '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]   temp1 = districts_df.groupby('locale')['pct_black/hispanic'].value_counts().to_frame() temp1.columns = ['amount']  temp1 = temp1.reset_index(drop=False) temp1 = temp1.pivot(index='locale', columns='pct_black/hispanic')['amount']  temp2 = districts_df.groupby('locale')['pct_free/reduced'].value_counts().to_frame() temp2.columns = ['amount']  temp2 = temp2.reset_index(drop=False)  temp2 = temp2.pivot(index='locale', columns='pct_free/reduced')['amount']  plt.style.use('default')  fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))  sns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d') ax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90); ax1.xaxis.set_tick_params(labelsize=16)  for container in ax1.containers:     ax1.bar_label(container,fontsize=16)  sns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14}) ax2.set_title('Heatmap: locale and pct_black/hispanic',fontsize=16,loc='left') ax2.xaxis.set_tick_params(labelsize=16) ax2.yaxis.set_tick_params(labelsize=16)  sns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14}) ax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left') ax3.xaxis.set_tick_params(labelsize=16) ax3.yaxis.set_tick_params(labelsize=16)  sns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14}) ax4.set_title('Heatmap: locale and pct_free/reduced',fontsize=16,loc='left') ax4.xaxis.set_tick_params(labelsize=16) ax4.yaxis.set_tick_params(labelsize=16)   plt.show() In\u00a0[6]: hide-input Copied! <pre>plt.style.use('default')\nnames = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12']\ncounts = [products_df[x].sum() for x in names]\n\ntemp_bar = pd.DataFrame({\n    'sector':names,\n    'count':counts\n}).sort_values('count',ascending = False)\n\ntemp = products_df.groupby('primary_function_main')[names].sum()\n\n#fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nplt.figure(figsize=(18,18))\n\nplt.subplot(3,2,1)\nax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d')\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=12)\n\nplt.subplot(3,2,2)\n\nsns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')\n\nplt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);\n\nplt.show()\n</pre> plt.style.use('default') names = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12'] counts = [products_df[x].sum() for x in names]  temp_bar = pd.DataFrame({     'sector':names,     'count':counts }).sort_values('count',ascending = False)  temp = products_df.groupby('primary_function_main')[names].sum()  #fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6)) plt.figure(figsize=(18,18))  plt.subplot(3,2,1) ax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d') for container in ax.containers:     ax.bar_label(container,fontsize=12)  plt.subplot(3,2,2)  sns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')  plt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);  plt.show() In\u00a0[7]: hide-input Copied! <pre># pieplot: products\n\ncolor = [\n    'darkgray',\n    'silver',\n    'lightgray',\n    'gainsboro',\n]\n\nproducts_df[\"primary_function_main\"].value_counts().plot(\n    kind = 'pie', \n    autopct='%1d%%', \n    figsize=(6,6), \n    colors=color,\n    wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},\n    textprops={'color':\"black\"},\n    startangle=0)\nplt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90);\nplt.show()\n</pre> # pieplot: products  color = [     'darkgray',     'silver',     'lightgray',     'gainsboro', ]  products_df[\"primary_function_main\"].value_counts().plot(     kind = 'pie',      autopct='%1d%%',      figsize=(6,6),      colors=color,     wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},     textprops={'color':\"black\"},     startangle=0) plt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90); plt.show() In\u00a0[8]: hide-input Copied! <pre># pieplot: products -&gt; subcategories\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\ntemp = products_df[products_df.primary_function_main == 'LC']\nax = sns.countplot(\n    data=temp, \n    y='primary_function_sub',\n    order=temp.primary_function_sub.value_counts().index,\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=16)\n\n\n#plt.title('Sub-Categories in Primary Function LC')\nplt.text(x = -50, y = -0.8, \n         s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);\n\nplt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\n\nplt.show()\n</pre> # pieplot: products -&gt; subcategories  plt.style.use('default') plt.figure(figsize=(18,8))  temp = products_df[products_df.primary_function_main == 'LC'] ax = sns.countplot(     data=temp,      y='primary_function_sub',     order=temp.primary_function_sub.value_counts().index,     palette ='Greys_d' )  for container in ax.containers:     ax.bar_label(container,fontsize=16)   #plt.title('Sub-Categories in Primary Function LC') plt.text(x = -50, y = -0.8,           s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);  plt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16) plt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16) plt.yticks(fontsize=16) plt.xticks(fontsize=16)  plt.show() In\u00a0[9]: hide-input Copied! <pre>dct = {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n}\n\ntemp = products_df['provider/company_name'].value_counts().reset_index()\ntemp.columns = ['provider/company_name','count']\ntemp = temp.replace( {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n})\n\nn = 15\ntemp = temp.sort_values('count',ascending = False).head(n)\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\nax = sns.barplot(\n    data=temp, \n    y='provider/company_name',\n    x='count',\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=15)\n    \nplt.text(x = -7, y = -1, \n         s = f\"Top {n} provider/company name\",fontsize = 20, weight = 'bold', alpha = .90);\n   \nplt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\n\n\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n</pre> dct = {     'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company' }  temp = products_df['provider/company_name'].value_counts().reset_index() temp.columns = ['provider/company_name','count'] temp = temp.replace( {     'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company' })  n = 15 temp = temp.sort_values('count',ascending = False).head(n)  plt.style.use('default') plt.figure(figsize=(18,8))  ax = sns.barplot(     data=temp,      y='provider/company_name',     x='count',     palette ='Greys_d' )  for container in ax.containers:     ax.bar_label(container,fontsize=15)      plt.text(x = -7, y = -1,           s = f\"Top {n} provider/company name\",fontsize = 20, weight = 'bold', alpha = .90);     plt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16) plt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16) plt.yticks(fontsize=16)   plt.yticks(fontsize=16) plt.xticks(fontsize=16) plt.show() <p>With regard to products, there are about 372 different products.</p> <p>We can make a word cloud to be able to analyze in a different way, words by themselves that are repeated the most in the <code>product_name</code> variable.</p> In\u00a0[10]: hide-input Copied! <pre>cloud = WordCloud(\n    width=1080,\n    height=270,\n    colormap='Greys',\n    background_color='white'\n    ).generate(\" \".join(products_df['product_name'].astype(str)))\n\nplt.figure(figsize=(22, 10))\nplt.imshow(cloud)\nplt.axis('off');\n</pre> cloud = WordCloud(     width=1080,     height=270,     colormap='Greys',     background_color='white'     ).generate(\" \".join(products_df['product_name'].astype(str)))  plt.figure(figsize=(22, 10)) plt.imshow(cloud) plt.axis('off'); <p>To understand more in detail the use of these products, we will analyze the use of these products with respect to the variable <code>engagement_index</code>. The first graph is related to the average <code>engagement_index</code> (per student) for the year 2020, where the first 15 products will be displayed.</p> <p>An important fact is that 362 products have an average of less than 1!.</p> In\u00a0[11]: hide-input Copied! <pre>group_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\ngroup_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2))\nless_1 = len(group_01.loc[lambda x:x['engagement_index']&lt;1])\n\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.barplot(\n    y=\"product_name\",\n    x = \"engagement_index\",\n    data=group_01.head(20),\n    palette=\"Greys_d\",\n\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=14)\n\nplt.text(x = -3.5, y = -3, \n         s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);\n\nplt.text(x = -3.5, y = -2, \n         s = \"per 1 student\",fontsize = 14,  alpha = .90);\n\nplt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14)\nplt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n</pre> group_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False) group_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2)) less_1 = len(group_01.loc[lambda x:x['engagement_index']&lt;1])   plt.style.use('default') plt.figure(figsize=(14,8))  plotting = sns.barplot(     y=\"product_name\",     x = \"engagement_index\",     data=group_01.head(20),     palette=\"Greys_d\",  )  for container in plotting.containers:     plotting.bar_label(container,fontsize=14)  plt.text(x = -3.5, y = -3,           s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);  plt.text(x = -3.5, y = -2,           s = \"per 1 student\",fontsize = 14,  alpha = .90);  plt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14) plt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14) plt.yticks(fontsize=16) plt.xticks(fontsize=16) plt.show() <p>Let's study the temporal behavior (at the level of weeks) of these tools during the year 2020, where the most  three used tools will be shown with different colors, while the other tools will be visualized but with the same color (in order to understand their distribution).</p> <p>Note: The proposed analysis can be carried out at the day level and analyzing through time series each of the tools during the year 2020.</p> In\u00a0[12]: hide-input Copied! <pre>col = 'week'\n\ngroup_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\n\ng_high = group_01.head(3)['product_name']\ngroup_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]\n\nstates = group_04['product_name'].unique()\ntimes= group_04[col].unique()\n\nindex = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])\n\ndf_complete = pd.DataFrame(index = index).reset_index().fillna(0)\n\ngroup_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)\n\nn = 3\ng_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()\n\n\ncolors = [    \n    'lightgray', \n    'dimgray', \n    'black', \n    'firebrick', \n    'darkred']\npalette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high}\npalette_02 = {g_high[i]:colors[i] for i in range(n)}\n\nplt.style.use('default')\nplt.figure(figsize=(20,6))\n\n\nsns.lineplot(\n    data=group_04.loc[lambda x: ~x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    legend = False,\n    palette=palette_01,\n    linewidth = 1.\n\n    )\n\nsns.lineplot(\n    data=group_04.loc[lambda x: x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    palette=palette_02,\n    linewidth = 1.\n\n    )\n\n\nplt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14)\nplt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)\n\n\nplt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8)\nplt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)\n\n\nplt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5)\nplt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5)\nplt.show()\n</pre> col = 'week'  group_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)  g_high = group_01.head(3)['product_name'] group_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]  states = group_04['product_name'].unique() times= group_04[col].unique()  index = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])  df_complete = pd.DataFrame(index = index).reset_index().fillna(0)  group_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)  n = 3 g_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()   colors = [         'lightgray',      'dimgray',      'black',      'firebrick',      'darkred'] palette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high} palette_02 = {g_high[i]:colors[i] for i in range(n)}  plt.style.use('default') plt.figure(figsize=(20,6))   sns.lineplot(     data=group_04.loc[lambda x: ~x.product_name.isin(g_high)],      x=col,      y=\"engagement_index\",      hue='product_name',     legend = False,     palette=palette_01,     linewidth = 1.      )  sns.lineplot(     data=group_04.loc[lambda x: x.product_name.isin(g_high)],      x=col,      y=\"engagement_index\",      hue='product_name',     palette=palette_02,     linewidth = 1.      )   plt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14) plt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)   plt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8) plt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)   plt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5) plt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5) plt.show() <p>Now, we can understand the <code>engagement index</code> for the most important tools about districts, where the districts of * Wisconsin *, * Missouri * and * Virginia * have the highest <code>engagement index</code> among the three most used tools.</p> In\u00a0[13]: hide-input Copied! <pre>group_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()/1000)\\\n            .reset_index().sort_values('engagement_index',ascending = False).fillna(0)\n\ngripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)]\ngripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2))\n#gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']&gt;0]\n\n\nplt.style.use('default')\n\ng = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  )\ng.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)\n\ng.fig.set_size_inches(15, 8)\ng.fig.subplots_adjust(top=0.81, right=0.86)\n\naxes = g.axes.flatten()\nfor ax in axes:\n    for container in ax.containers:\n        ax.bar_label(container,fontsize=8)\n\n\nplt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90);\nplt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90);\nplt.show()\n</pre> group_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()/1000)\\             .reset_index().sort_values('engagement_index',ascending = False).fillna(0)  gripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)] gripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2)) #gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']&gt;0]   plt.style.use('default')  g = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  ) g.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)  g.fig.set_size_inches(15, 8) g.fig.subplots_adjust(top=0.81, right=0.86)  axes = g.axes.flatten() for ax in axes:     for container in ax.containers:         ax.bar_label(container,fontsize=8)   plt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90); plt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90); plt.show()"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#impact-on-digital-learning","title":"Impact on Digital Learning\u00b6","text":"<p>Main objective is understand of the best way the challenge LearnPlatform COVID-19 Impact on Digital Learning proposed by Kaggle.</p> <p>The steps to follow are:</p> <ul> <li>Overview of the Dataset: Understanding the datasets available.</li> <li>Preprocessing: Preprocessing of the datasets available.</li> <li>EDA: Exploratory data analysis using visualization tools in Python.</li> </ul> <p>Note: My analysis is inspired by several of the notebooks that different profiles have uploaded to the challenge, so some graphics or images belong to these authors. The most important ones will be found in the references. On the other hand,  my project is available in Jupyter Book, click in the following link.</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#overview-of-the-dataset","title":"Overview of the Dataset\u00b6","text":"<p>The objective of this section is to be able to read and give an interpretation to each one of the available datasets, analyzing column by column. For each dataset we will make a brief description:</p> <ul> <li>File: File name (<code>.csv</code>).</li> <li>Shape: Dimensionality of datasets.</li> <li>Description: Basic description of the dataset.</li> <li>Top 5 rows: Show first 5 rows + explanation for some columns.</li> <li>Summary: Summary of datasets.</li> </ul>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#1-districts","title":"1.  Districts\u00b6","text":"<ul> <li>File: <code>districts_info.csv</code>.</li> <li>Shape: $233$ rows $\\times$  $7$ columns.</li> <li>Description: file contains information about each school district.</li> <li>Top 5 rows::</li> </ul>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#2-products","title":"2. Products\u00b6","text":"<ul> <li>File: <code>products_info.csv</code></li> <li>Shape: $372$ rows $\\times$  $6$ columns.</li> <li>Description: for each school district, there is an additional file that contains the engagement for each tool for everyday in 2020.</li> <li>Top 5 rows:: </li> </ul>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#3-engagement","title":"3. Engagement\u00b6","text":"<ul> <li>File: <code>engagement_data/*.csv</code>.</li> <li>Shape: $22324190$ rows $\\times$  $5$ columns.</li> <li>Description: file contains information about each school district. The files can be joined by the key columns <code>district_id</code> and <code>lp_id</code>.</li> <li>Top 5 rows:: </li> </ul>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#preprocessing","title":"Preprocessing\u00b6","text":"<p>Preprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that the way I preprocess the data may not be suited for your analysis purposes. Therefore, before you begin preprocessing your data, think about which data you would like to keep and/or modify and which data is not relevant for your analysis.</p> <ul> <li>one-hot encoding the product sectors</li> <li>splitting up the primary essential function into main and sub category</li> </ul> <p>Note: Preprocessing varies if you see other notebooks of this challenge. The processing will depend on the understanding of each of the datasets and the extra information that you may have.</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#eda","title":"EDA\u00b6","text":"<p>Exploratory data analysis is the most important part of the challenge, since this will make the difference between the winner and the other participants. You should keep in mind that your visualizations must be able to simply and easily summarize the datasets. Also, it is hoped that the proposed visualizations can help to understand behaviors that are not easy to analyze with a simple table.</p> <p>Visualizations will be made in matplotlib, seaborn y plotly. Based on the article by Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland, we will occupy <code>Grays</code> scale  next to the technique: dark text on a light background.</p> <p>Note: Visualizations made on this notebook are static. You can use different tools to be able to make dynamic visualizations (Altair, plotly, etc.). You can also perform tools like Streamlit to make Dashboards. On the other hand, if you fully understand python visualization tools and have knowledge of HTML/CSS, you can make beautiful notebook presentations like this one.</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#visualization-districts","title":"Visualization: Districts\u00b6","text":"<p>First of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. . The states with the most available school districts are CT (30) and UT (29) while there are also states with only one school district (FL, TN, NY, AZ).</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#visualization-locales","title":"Visualization: Locales\u00b6","text":"<p>Locales are separated into 4 categories: Suburb,Rural, City and Town, where most of the locales are concentrated in the Suburb category (104).</p> <p>For the <code>pct_black/hispanic</code> variable, Rural and Town categories concentrate their entire population close to the interval $ [0,0.2 [$, while for the others sectors this percentage is varied.</p> <p>For <code>pctfree/reduced</code> and <code>pp_total_raw</code> indicators, the distribution for each location is different, although they tend to focus on a particular interval.</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#visualization-sectors","title":"Visualization: Sectors\u00b6","text":"<p>Sectors are separated into 3 categories: sector_Corporate, sector_HigherEd and sector_PreK-12, donde la categor\u00eda mayoritaria corresponde a sector_PreK-12 (350). On the other hand, analyzing the <code>primary_function_main</code> variable, all sectors are focused on the<code> LC</code> category. It is worth mentioning that the distribution of the other categories remains almost the same between sectors.</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#visualization-primary_function_main","title":"Visualization:  primary_function_main\u00b6","text":"<p>Continuing the analysis of the <code>primary_function_main</code> variable, it was observed that most of these are in the<code> LC</code> category (77%). Within this category, its subcategory is analyzed, where the predominant subcategory is <code>Sites, Resources &amp; Reference</code> (101).</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#visualization-products","title":"Visualization: Products\u00b6","text":"<p>After understanding the functionality of each of the tools, it is necessary to understand the distribution of the tools. The first thing is to study the distribution of the providers of the products we have, where:</p> <ul> <li>258 providers have 1 occurrences.</li> <li>18 providers have 2 occurrences.</li> <li>9 providers have 3 occurrences.</li> <li>2 providers have 4 occurrences.</li> <li>2 providers have 6 occurrences.</li> <li>1 provider have 30 occurrences.</li> </ul> <p>Based on this, only the top 15 providers will be displayed.</p>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#summary","title":"Summary\u00b6","text":"<ul> <li>Depending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.</li> <li>When looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning</li> </ul>"},{"location":"blog/2021/basic-analysis-impact-on-digital-learning/#references","title":"References\u00b6","text":"<ul> <li><p>Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland</p> </li> <li><p>Kaggle Competitions:</p> <ul> <li>Enthusiast to Data Professional - What changes?</li> <li>How To Approach Analytics Challenges</li> <li>Most popular tools in 2020 Digital Learning</li> </ul> </li> </ul>"},{"location":"blog/2022/2021-07-15-tdd/","title":"TDD","text":"<pre>@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\n</pre> <p>El test nos dice que si el input es el n\u00famero <code>2</code>, la funci\u00f3n <code>paridad</code> devuelve el output <code>'par'</code>. C\u00f3mo a\u00fan no hemos escrito la funci\u00f3n, el test fallar\u00e1 (fase red).</p> <pre><code>========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py F                                              [100%]\n========= 1 failed in 0.14s  ===============================================\n</code></pre> <p>Ahora, se escribe la funci\u00f3n <code>paridad</code> (fase green):</p> <pre>def paridad(n:int)-&gt;str:\n\"\"\"\n    Determina si un numero natural es par o no.\n    :param n: numero entero\n    :return: 'par' si es el numero es par; 'impar' en otro caso\n    \"\"\"\n    return 'par' if n%2==0 else 'impar'\n</pre> <p>Volvemos a correr el test:</p> <pre><code>========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py .                                              [100%]\n========= 1 passed in 0.06s  ===============================================\n</code></pre> <p>Hemos cometido un descuido a proposito, no hemos testeado el caso si el n\u00famero fuese impar, por lo cual reescribimos el test (fase refactor)</p> <pre>@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n        (3, 'impar'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\n</pre> <p>y corremos nuevamente los test:</p> <pre><code>========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 2 items                                                                                                                                                                          \n\ntemp/test_funcion.py ..                                              [100%]\n========= 2 passed in 0.06s  ===============================================\n</code></pre> <p>Listo, nuestra funci\u00f3n <code>paridad</code> ha sido testeado correctamente!.</p> <p>Como todo en la vida, nada es gratis:</p> <p>Incremento del tiempo de desarrollo var\u00eda entre un 15% a 35%.</p> <p>Sin embargo</p> <p>Desde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantenci\u00f3n reducidos debido al incremento en calidad.</p> <p>Adem\u00e1s, es importante escribir tests junto con la implementaci\u00f3n en peque\u00f1as iteraciones. George y Williams encontraron que escribir tests despu\u00e9s de que la aplicaci\u00f3n est\u00e1 mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y adem\u00e1s la aplicaci\u00f3n se vuelve menos testeable. Otra conclusi\u00f3n interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un dise\u00f1o m\u00e1s simple.</p>"},{"location":"blog/2022/2021-07-15-tdd/#tdd","title":"TDD\u00b6","text":""},{"location":"blog/2022/2021-07-15-tdd/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Esta secci\u00f3n busca dar se\u00f1ales de c\u00f3mo abordar el desarrollo de software para Data Science usando Test Driven Development, una t\u00e9cnica ampliamente usada en otros rubros de la programaci\u00f3n.</p>"},{"location":"blog/2022/2021-07-15-tdd/#que-es-el-test-driven-development","title":"\u00bfQu\u00e9 es el Test Driven Development?\u00b6","text":"<p>En palabras simples, el desarrollo guiado por pruebas pone las pruebas en el coraz\u00f3n de nuestro trabajo. En su forma m\u00e1s simple consiste en un proceso iterativo de 3 fases:</p> <p></p> <ul> <li>Red: Escribe un test que ponga a prueba una nueva funcionalidad y asegurate de que el test falla</li> <li>Green: Escribe el c\u00f3digo m\u00ednimo necesario para pasar ese test</li> <li>Refactor: Refactoriza de ser necesario</li> </ul>"},{"location":"blog/2022/2021-07-15-tdd/#ejemplo-sencillo","title":"Ejemplo sencillo\u00b6","text":"<p>A modo de ejemplo, vamos a testear la funci\u00f3n <code>paridad</code>, que determina si un n\u00famero natural es par o no.</p> <p>Lo primero que se debe hacer es crear el test, para ello se ocupar\u00e1 la librer\u00eda pytest.</p> <p>Nota: No es necesario conocer previamente la librer\u00eda <code>pytest</code> para entender el ejemplo.</p>"},{"location":"blog/2022/2021-07-15-tdd/#porque-deberia-usarlo","title":"\u00bfPorqu\u00e9 deber\u00eda usarlo?\u00b6","text":"<p>Existen varias razones por las que uno deber\u00eda usar TDD. Entre ellas podemos encontrar:</p> <ul> <li>Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los l\u00edmites del problema y c\u00f3mo podemos resolverlo. Con el tiempo esto ayuda a obtener un dise\u00f1o modular y reusable del c\u00f3digo.</li> <li>Escribir tests ayuda la forma en que escribimos c\u00f3digo, haci\u00e9ndolo m\u00e1s legible a otros. Sin embargo, no es un acto de altruismo, la mayor\u00eda de las veces ese otro es tu futuro yo.</li> <li>Verifica que el c\u00f3digo funciona de la manera que se espera, y lo hace de forma autom\u00e1tica.</li> <li>Te permite realizar refactoring con la certeza de que no has roto nada.</li> <li>Los tests escritos sirven como documentaci\u00f3n para otros desarrolladores.</li> <li>Es una pr\u00e1ctica requerida en metodolog\u00edas de desarrollo de software agile.</li> </ul>"},{"location":"blog/2022/2021-07-15-tdd/#evidencia-empirica","title":"Evidencia emp\u00edrica\u00b6","text":"<p>El 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de c\u00f3digo hasta las 155k. Estas son parte de sus conclusiones:</p> <p>Todos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft.</p>"},{"location":"blog/2022/2021-07-15-tdd/#puedo-usar-tdd-siempre","title":"\u00bfPuedo usar TDD siempre?\u00b6","text":"<p>No, pero puedes usarlo casi siempre. El an\u00e1lisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementaci\u00f3n v\u00eda testing.</p>"},{"location":"blog/2022/2021-07-15-tdd/#librerias-disponibles","title":"Librer\u00edas disponibles\u00b6","text":"<p>Ac\u00e1 listamos algunas librer\u00edas de TDD en Python:</p> <ul> <li>unittest: M\u00f3dulo dentro de la librer\u00eda est\u00e1ndar de Python. Permite realizar tests unitarios, de integraci\u00f3n y end to end.</li> <li>doctest: Permite realizar test de la documentaci\u00f3n del c\u00f3digo (ejemplos: Numpy o Pandas).</li> <li>pytest: Librer\u00eda de testing ampliamente usada en proyectos nuevos de Python.</li> <li>nose: Librer\u00eda que extiende unittest para hacerlo m\u00e1s simple.</li> <li>coverage: Herramienta para medir la cobertura de c\u00f3digo de los proyectos.</li> <li>tox: Herramienta para facilitar el test de una librer\u00eda en diferentes versiones e int\u00e9rpretes de Python.</li> <li>hypothesis: Librer\u00eda para escribir tests v\u00eda reglas que ayuda a encontrar casos borde.</li> <li>behave: Permite utilizar Behavior Driven Development, un proceso de desarrollo derivado del TDD.</li> </ul>"},{"location":"blog/2022/2021-07-15-tdd/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, es una buena lectura, sobretodo los consejos que dan en las conclusiones.</li> <li>Google Testing Blog: Poseen varios art\u00edculos sobre c\u00f3mo abordar problemas tipo, buenas pr\u00e1cticas de dise\u00f1o para generar c\u00f3digo testeable, entre otros. En particular destaca la serie Testing on the Toilet.</li> <li>Cualquier art\u00edculo de Martin Fowler sobre testing, empezando por \u00e9ste</li> <li>Design Patterns: Los patrones de dise\u00f1o de software tienen en consideraci\u00f3n que el c\u00f3digo sea testeable.</li> </ul>"},{"location":"blog/2022/2022-03-16-polars/","title":"Polars","text":"In\u00a0[1]: Copied! <pre>import polars as pl\nimport numpy as np\n</pre> import polars as pl import numpy as np In\u00a0[2]: Copied! <pre>np.random.seed(12)\n\ndf = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n</pre> np.random.seed(12)  df = pl.DataFrame(     {         \"nrs\": [1, 2, 3, None, 5],         \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],         \"random\": np.random.rand(5),         \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],     } ) print(df) <pre>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2    \u2506 ham   \u2506 0.74     \u2506 A      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>Puedes hacer mucho con las expresiones, veamos algunos ejemplos:</p> In\u00a0[3]: Copied! <pre>out = df.select(\n    [\n        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n    ]\n)\nprint(out)\n</pre> out = df.select(     [         pl.col(\"names\").n_unique().alias(\"unique_names_1\"),         pl.col(\"names\").unique().count().alias(\"unique_names_2\"),     ] ) print(out) <pre>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 unique_names_1 \u2506 unique_names_2 \u2502\n\u2502 ---            \u2506 ---            \u2502\n\u2502 u32            \u2506 u32            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5              \u2506 5              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[4]: Copied! <pre>out = df.select(\n    [\n        pl.sum(\"random\").alias(\"sum\"),\n        pl.min(\"random\").alias(\"min\"),\n        pl.max(\"random\").alias(\"max\"),\n        pl.col(\"random\").max().alias(\"other_max\"),\n        pl.std(\"random\").alias(\"std dev\"),\n        pl.var(\"random\").alias(\"variance\"),\n    ]\n)\nprint(out)\n</pre> out = df.select(     [         pl.sum(\"random\").alias(\"sum\"),         pl.min(\"random\").alias(\"min\"),         pl.max(\"random\").alias(\"max\"),         pl.col(\"random\").max().alias(\"other_max\"),         pl.std(\"random\").alias(\"std dev\"),         pl.var(\"random\").alias(\"variance\"),     ] ) print(out) <pre>shape: (1, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sum      \u2506 min      \u2506 max  \u2506 other_max \u2506 std dev  \u2506 variance \u2502\n\u2502 ---      \u2506 ---      \u2506 ---  \u2506 ---       \u2506 ---      \u2506 ---      \u2502\n\u2502 f64      \u2506 f64      \u2506 f64  \u2506 f64       \u2506 f64      \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.705842 \u2506 0.014575 \u2506 0.74 \u2506 0.74      \u2506 0.293209 \u2506 0.085971 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[5]: Copied! <pre>out = df.select(\n    [\n        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"am$\")).count(),\n    ]\n)\nprint(out)\n</pre> out = df.select(     [         pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"am$\")).count(),     ] ) print(out) <pre>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 names \u2502\n\u2502 ---   \u2502\n\u2502 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[6]: Copied! <pre>out = df.select(\n    [\n        pl.when(pl.col(\"random\") &gt; 0.5).then(0).otherwise(pl.col(\"random\")) * pl.sum(\"nrs\"),\n    ]\n)\nprint(out)\n</pre> out = df.select(     [         pl.when(pl.col(\"random\") &gt; 0.5).then(0).otherwise(pl.col(\"random\")) * pl.sum(\"nrs\"),     ] ) print(out) <pre>shape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 literal  \u2502\n\u2502 ---      \u2502\n\u2502 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.695791 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 0.0      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2.896465 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 0.0      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 0.160325 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[7]: Copied! <pre>out = df[\n    [\n        pl.col(\"*\"),  # select all\n        pl.col(\"random\").sum().over(\"groups\").alias(\"sum[random]/groups\"),\n        pl.col(\"random\").list().over(\"names\").alias(\"random/name\"),\n    ]\n]\nprint(out)\n</pre> out = df[     [         pl.col(\"*\"),  # select all         pl.col(\"random\").sum().over(\"groups\").alias(\"sum[random]/groups\"),         pl.col(\"random\").list().over(\"names\").alias(\"random/name\"),     ] ] print(out) <pre>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2506 sum[random]/groups \u2506 random/name \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2506 ---                \u2506 ---         \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2506 f64                \u2506 list [f64]  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2506 0.894213           \u2506 [0.154163]  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2    \u2506 ham   \u2506 0.74     \u2506 A      \u2506 0.894213           \u2506 [0.74]      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2506 0.2778             \u2506 [0.263315]  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2506 0.533739           \u2506 [0.533739]  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2506 0.2778             \u2506 [0.014575]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>Para las operaciones hash realizadas durante la fase de \"divisi\u00f3n\", Polars utiliza un enfoque sin bloqueo de subprocesos m\u00faltiples que se ilustra en el siguiente esquema:</p> <p></p> <p>\u00a1Esta paralelizaci\u00f3n permite que las operaciones de agrupaci\u00f3n y uni\u00f3n (por ejemplo) sean incre\u00edblemente r\u00e1pidas!</p> In\u00a0[8]: Copied! <pre>import polars as pl\n\ndataset = pl.read_csv(\"legislators-current.csv\")\ndataset = dataset.with_column(pl.col(\"birthday\").str.strptime(pl.Date))\nprint(dataset.head())\n</pre> import polars as pl  dataset = pl.read_csv(\"legislators-current.csv\") dataset = dataset.with_column(pl.col(\"birthday\").str.strptime(pl.Date)) print(dataset.head()) <pre>shape: (5, 34)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 last_name \u2506 first_name \u2506 middle_name \u2506 suffix \u2506 ... \u2506 ballotpedia_id \u2506 washington_post_id \u2506 icpsr_id \u2506 wikipedia_id   \u2502\n\u2502 ---       \u2506 ---        \u2506 ---         \u2506 ---    \u2506     \u2506 ---            \u2506 ---                \u2506 ---      \u2506 ---            \u2502\n\u2502 str       \u2506 str        \u2506 str         \u2506 str    \u2506     \u2506 str            \u2506 str                \u2506 i64      \u2506 str            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Brown     \u2506 Sherrod    \u2506 null        \u2506 null   \u2506 ... \u2506 Sherrod Brown  \u2506 null               \u2506 29389    \u2506 Sherrod Brown  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Cantwell  \u2506 Maria      \u2506 null        \u2506 null   \u2506 ... \u2506 Maria Cantwell \u2506 null               \u2506 39310    \u2506 Maria Cantwell \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Cardin    \u2506 Benjamin   \u2506 L.          \u2506 null   \u2506 ... \u2506 Ben Cardin     \u2506 null               \u2506 15408    \u2506 Ben Cardin     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Carper    \u2506 Thomas     \u2506 Richard     \u2506 null   \u2506 ... \u2506 Tom Carper     \u2506 null               \u2506 15015    \u2506 Tom Carper     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Casey     \u2506 Robert     \u2506 P.          \u2506 Jr.    \u2506 ... \u2506 Bob Casey, Jr. \u2506 null               \u2506 40703    \u2506 Bob Casey Jr.  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[9]: Copied! <pre>q = (\n    dataset.lazy()\n    .groupby(\"first_name\")\n    .agg(\n        [\n            pl.count(),\n            pl.col(\"gender\").list(),\n            pl.first(\"last_name\"),\n        ]\n    )\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</pre> q = (     dataset.lazy()     .groupby(\"first_name\")     .agg(         [             pl.count(),             pl.col(\"gender\").list(),             pl.first(\"last_name\"),         ]     )     .sort(\"count\", reverse=True)     .limit(5) )  df = q.collect() print(df) <pre>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 first_name \u2506 count \u2506 gender              \u2506 last_name \u2502\n\u2502 ---        \u2506 ---   \u2506 ---                 \u2506 ---       \u2502\n\u2502 str        \u2506 u32   \u2506 list [str]          \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 John       \u2506 19    \u2506 [\"M\", \"M\", ... \"M\"] \u2506 Barrasso  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Mike       \u2506 13    \u2506 [\"M\", \"M\", ... \"M\"] \u2506 Kelly     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Michael    \u2506 11    \u2506 [\"M\", \"M\", ... \"M\"] \u2506 Bennet    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 David      \u2506 11    \u2506 [\"M\", \"M\", ... \"M\"] \u2506 Cicilline \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 James      \u2506 9     \u2506 [\"M\", \"M\", ... \"M\"] \u2506 Inhofe    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[10]: Copied! <pre>q = (\n    dataset.lazy()\n    .groupby(\"state\")\n    .agg(\n        [\n            (pl.col(\"party\") == \"Democrat\").sum().alias(\"demo\"),\n            (pl.col(\"party\") == \"Republican\").sum().alias(\"repu\"),\n        ]\n    )\n    .sort(\"demo\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</pre> q = (     dataset.lazy()     .groupby(\"state\")     .agg(         [             (pl.col(\"party\") == \"Democrat\").sum().alias(\"demo\"),             (pl.col(\"party\") == \"Republican\").sum().alias(\"repu\"),         ]     )     .sort(\"demo\", reverse=True)     .limit(5) )  df = q.collect() print(df) <pre>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 demo \u2506 repu \u2502\n\u2502 ---   \u2506 ---  \u2506 ---  \u2502\n\u2502 str   \u2506 u32  \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CA    \u2506 44   \u2506 10   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 NY    \u2506 21   \u2506 8    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 IL    \u2506 15   \u2506 5    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 TX    \u2506 13   \u2506 25   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 NJ    \u2506 12   \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>Por supuesto, tambi\u00e9n se podr\u00eda hacer algo similar con un <code>GROUPBY</code> anidado, pero eso no me permitir\u00eda mostrar estas caracter\u00edsticas agradables. \ud83d\ude09</p> In\u00a0[11]: Copied! <pre>q = (\n    dataset.lazy()\n    .groupby([\"state\", \"party\"])\n    .agg([pl.count(\"party\").alias(\"count\")])\n    .filter((pl.col(\"party\") == \"Democrat\") | (pl.col(\"party\") == \"Republican\"))\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</pre> q = (     dataset.lazy()     .groupby([\"state\", \"party\"])     .agg([pl.count(\"party\").alias(\"count\")])     .filter((pl.col(\"party\") == \"Democrat\") | (pl.col(\"party\") == \"Republican\"))     .sort(\"count\", reverse=True)     .limit(5) )  df = q.collect() print(df) <pre>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 party      \u2506 count \u2502\n\u2502 ---   \u2506 ---        \u2506 ---   \u2502\n\u2502 str   \u2506 str        \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CA    \u2506 Democrat   \u2506 44    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 TX    \u2506 Republican \u2506 25    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 NY    \u2506 Democrat   \u2506 21    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 FL    \u2506 Republican \u2506 18    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 IL    \u2506 Democrat   \u2506 15    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[12]: Copied! <pre>from datetime import date\n\ndef compute_age() -&gt; pl.Expr:\n    return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -&gt; pl.Expr:\n    return compute_age().filter(pl.col(\"gender\") == gender).mean().alias(f\"avg {gender} birthday\")\n</pre> from datetime import date  def compute_age() -&gt; pl.Expr:     return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()   def avg_birthday(gender: str) -&gt; pl.Expr:     return compute_age().filter(pl.col(\"gender\") == gender).mean().alias(f\"avg {gender} birthday\") In\u00a0[13]: Copied! <pre>q = (\n    dataset.lazy()\n    .groupby([\"state\"])\n    .agg(\n        [\n            avg_birthday(\"M\"),\n            avg_birthday(\"F\"),\n            (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n            (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</pre> q = (     dataset.lazy()     .groupby([\"state\"])     .agg(         [             avg_birthday(\"M\"),             avg_birthday(\"F\"),             (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),             (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),         ]     )     .limit(5) )  df = q.collect() print(df) <pre>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 avg M birthday \u2506 avg F birthday \u2506 # male \u2506 # female \u2502\n\u2502 ---   \u2506 ---            \u2506 ---            \u2506 ---    \u2506 ---      \u2502\n\u2502 str   \u2506 f64            \u2506 f64            \u2506 u32    \u2506 u32      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 MS    \u2506 60.0           \u2506 62.0           \u2506 5      \u2506 1        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 NV    \u2506 55.5           \u2506 61.75          \u2506 2      \u2506 4        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 KS    \u2506 54.2           \u2506 41.0           \u2506 5      \u2506 1        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 IN    \u2506 55.0           \u2506 50.5           \u2506 9      \u2506 2        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 IL    \u2506 60.923077      \u2506 58.428571      \u2506 13     \u2506 7        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[14]: Copied! <pre>def get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\")\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\n\nprint(df)\n</pre> def get_person() -&gt; pl.Expr:     return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")   q = (     dataset.lazy()     .sort(\"birthday\")     .groupby([\"state\"])     .agg(         [             get_person().first().alias(\"youngest\"),             get_person().last().alias(\"oldest\"),         ]     )     .limit(5) )  df = q.collect()  print(df) <pre>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest                 \u2506 oldest                   \u2502\n\u2502 ---   \u2506 ---                      \u2506 ---                      \u2502\n\u2502 str   \u2506 str                      \u2506 str                      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 PR    \u2506 Jenniffer Gonz\u00e1lez-Col\u00f3n \u2506 Jenniffer Gonz\u00e1lez-Col\u00f3n \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 ND    \u2506 John Hoeven              \u2506 Kelly Armstrong          \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 KY    \u2506 Harold Rogers            \u2506 Garland Barr             \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 NM    \u2506 Teresa Leger Fernandez   \u2506 Melanie Stansbury        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 OR    \u2506 Peter DeFazio            \u2506 Jeff Merkley             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre>"},{"location":"blog/2022/2022-03-16-polars/#polars","title":"Polars\u00b6","text":""},{"location":"blog/2022/2022-03-16-polars/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Polars es una librer\u00eda de DataFrames incre\u00edblemente r\u00e1pida implementada en Rust utilizando Arrow Columnar Format de Apache como modelo de memoria.</p> <ul> <li>Lazy | eager execution</li> <li>Multi-threaded</li> <li>SIMD (Single Instruction, Multiple Data)</li> <li>Query optimization</li> <li>Powerful expression API</li> <li>Rust | Python | ...</li> </ul> <p>Esta secci\u00f3n tiene como objetivos presentarle Polars a trav\u00e9s de ejemplos y compar\u00e1ndolo con otras soluciones.</p> <p>Nota: Si usted no esta familiarizado con la manipulaci\u00f3n de datos en Python, se recomienda partir leyendo sobre la librer\u00eda de Pandas. Tambi\u00e9n, se deja como referencia el curso de Manipulaci\u00f3n de Datos.</p>"},{"location":"blog/2022/2022-03-16-polars/#primeros-pasos","title":"Primeros Pasos\u00b6","text":""},{"location":"blog/2022/2022-03-16-polars/#instalacion","title":"Instalaci\u00f3n\u00b6","text":"<p>Para instalar Polars, necesitar\u00e1 usar la l\u00ednea de comando. Si ha instalado Anaconda, puede usar:</p> <pre><code>conda install -c conda-forge polars\n</code></pre> <p>De lo contrario, puede instalar con pip:</p> <pre><code>pip install polars\n</code></pre> <p>Nota: Todos los binarios est\u00e1n preconstruidos para Python v3.6+.</p>"},{"location":"blog/2022/2022-03-16-polars/#rendimiento","title":"Rendimiento\u00b6","text":"<p>Polars es muy r\u00e1pido y, de hecho, es una de las mejores soluciones disponibles. Tomemos como referencia db-benchmark de h2oai. Esta p\u00e1gina tiene como objetivo comparar varias herramientas similares a bases de datos populares en la ciencia de datos de c\u00f3digo abierto. Se ejecuta regularmente con las \u00faltimas versiones de estos paquetes y se actualiza autom\u00e1ticamente.</p> <p>Tambi\u00e9n se incluye la sintaxis que se cronometra junto con el tiempo. De esta manera, puede ver de inmediato si est\u00e1 realizando estas tareas o no, y si las diferencias de tiempo le importan o no. Una diferencia de 10x puede ser irrelevante si eso es solo 1s frente a 0,1s en el tama\u00f1o de sus datos.</p> <p>A modo de ejemplo, veamos algunos ejemplos de performances de distintas librer\u00edas para ejecutar distintos tipos de tareas sobre datasets con distintos tama\u00f1os. Para el caso de tareas b\u00e1sicas sobre un dataset de 50 GB, Polars supera a librer\u00edas espacializadas en distribuci\u00f3n de Dataframes como Spark (143 segundos vs 568 segundos). Por otro lado, librer\u00edas conocidas en Python como Pandas o Dask se tiene el problema de out of memory.</p> <p></p>"},{"location":"blog/2022/2022-03-16-polars/#expresiones-en-polars","title":"Expresiones en Polars\u00b6","text":"<p>Polars tiene un poderoso concepto llamado expresiones. Las expresiones polares se pueden usar en varios contextos y son un mapeo funcional de <code>Fn(Series) -&gt; Series</code>, lo que significa que tienen <code>Series</code> como entrada y <code>Series</code> como salida. Al observar esta definici\u00f3n funcional, podemos ver que la salida de un <code>Expr</code> tambi\u00e9n puede servir como entrada de un <code>Expr</code>.</p> <p>Eso puede sonar un poco extra\u00f1o, as\u00ed que vamos a dar un ejemplo.</p> <p>La siguiente es una expresi\u00f3n:</p> <pre>pl.col(\"foo\").sort().head(2)\n</pre> <p>El fragmento anterior dice seleccionar la columna <code>\"foo\"</code>, luego ordenar esta columna y luego tomar los primeros 2 valores de la salida ordenada. El poder de las expresiones es que cada expresi\u00f3n produce una nueva expresi\u00f3n y que se pueden canalizar juntas. Puede ejecutar una expresi\u00f3n pas\u00e1ndola en uno de los contextos de ejecuci\u00f3n polares. Aqu\u00ed ejecutamos dos expresiones ejecutando <code>df.select</code>:</p> <pre>df.select([\n     pl.col(\"foo\").sort().head(2),\n     pl.col(\"barra\").filter(pl.col(\"foo\") == 1).sum()\n])\n</pre> <p>Todas las expresiones se ejecutan en paralelo. (Tenga en cuenta que dentro de una expresi\u00f3n puede haber m\u00e1s paralelizaci\u00f3n).</p>"},{"location":"blog/2022/2022-03-16-polars/#expresiones","title":"Expresiones\u00b6","text":"<p>En esta secci\u00f3n veremos algunos ejemplos, pero primero vamos a crear un conjunto de datos:</p>"},{"location":"blog/2022/2022-03-16-polars/#contar-valores-unicos","title":"Contar valores \u00fanicos\u00b6","text":"<p>Podemos contar los valores \u00fanicos en una columna. Tenga en cuenta que estamos creando el mismo resultado de diferentes maneras. Para no tener nombres de columna duplicados en el DataFrame, usamos una expresi\u00f3n de <code>alias</code>, que cambia el nombre de una expresi\u00f3n.</p>"},{"location":"blog/2022/2022-03-16-polars/#varias-agregaciones","title":"Varias agregaciones\u00b6","text":"<p>Podemos hacer varias agregaciones. A continuaci\u00f3n mostramos algunas de ellas, pero hay m\u00e1s, como <code>median</code>, <code>mean</code>, <code>first</code>, etc.</p>"},{"location":"blog/2022/2022-03-16-polars/#filtro-y-condicionales","title":"Filtro y condicionales\u00b6","text":"<p>Tambi\u00e9n podemos hacer cosas bastante complejas. En el siguiente fragmento, contamos todos los nombres que terminan con la cadena <code>\"am\"</code>.</p>"},{"location":"blog/2022/2022-03-16-polars/#funciones-binarias-y-modificacion","title":"Funciones binarias y modificaci\u00f3n\u00b6","text":"<p>En el ejemplo a continuaci\u00f3n, usamos un condicional para crear una nueva expresi\u00f3n <code>when -&gt; then -&gt; otherwise</code>.</p> <p>La funci\u00f3n <code>when()</code> requiere una expresi\u00f3n de predicado (y, por lo tanto, conduce a una <code>serie booleana</code>), luego espera una expresi\u00f3n que se usar\u00e1 en caso de que el predicado se eval\u00fae como verdadero y, de lo contrario, espera una expresi\u00f3n que se usar\u00e1 en caso de que el predicado se eval\u00fae.</p> <p>Tenga en cuenta que puede pasar cualquier expresi\u00f3n, o simplemente expresiones base como <code>pl.col(\"foo\")</code>, <code>pl.lit(3)</code>, <code>pl.lit(\"bar\")</code>, etc.</p> <p>Finalmente, multiplicamos esto con el resultado de una expresi\u00f3n de suma.</p>"},{"location":"blog/2022/2022-03-16-polars/#expresiones-de-ventana","title":"Expresiones de ventana\u00b6","text":"<p>Una expresi\u00f3n polar tambi\u00e9n puede hacer un <code>GROUPBY</code>, <code>AGGREGATION</code> y <code>JOIN</code> impl\u00edcitos en una sola expresi\u00f3n.</p> <p>En los ejemplos a continuaci\u00f3n, hacemos un <code>GROUPBY</code> sobre <code>\"groups\"</code> y <code>AGREGATE SUM</code> de <code>\"random\"</code>, y en la siguiente expresi\u00f3n <code>GROUPBY OVER</code> <code>\"names\"</code> y <code>AGREGATE</code> una lista de <code>\"random\"</code>. Estas funciones de ventana se pueden combinar con otras expresiones y son una forma eficaz de determinar estad\u00edsticas de grupo. Vea m\u00e1s expresiones en el siguiente link.</p>"},{"location":"blog/2022/2022-03-16-polars/#groupby","title":"GroupBy\u00b6","text":""},{"location":"blog/2022/2022-03-16-polars/#un-enfoque-multiproceso","title":"Un enfoque multiproceso\u00b6","text":"<p>Una de las formas m\u00e1s eficientes de procesar datos tabulares es paralelizar su procesamiento a trav\u00e9s del enfoque \"dividir-aplicar-combinar\". Esta operaci\u00f3n es el n\u00facleo de la implementaci\u00f3n del agrupamiento de Polars, lo que le permite lograr operaciones ultrarr\u00e1pidas. M\u00e1s espec\u00edficamente, las fases de \"divisi\u00f3n\" y \"aplicaci\u00f3n\" se ejecutan de forma multiproceso.</p> <p>Una operaci\u00f3n de agrupaci\u00f3n simple se toma a continuaci\u00f3n como ejemplo para ilustrar este enfoque:</p> <p></p>"},{"location":"blog/2022/2022-03-16-polars/#no-mates-la-paralelizacion","title":"\u00a1No mates la paralelizaci\u00f3n!\u00b6","text":"<p>Todos hemos escuchado que Python es lento y \"no escala\". Adem\u00e1s de la sobrecarga de ejecutar el c\u00f3digo de bytes \"lento\", Python debe permanecer dentro de las restricciones del Global interpreter lock (GIL). Esto significa que si se usa la operaci\u00f3n <code>lambda</code> o una funci\u00f3n de Python personalizada para aplicar durante una fase de paralelizaci\u00f3n, la velocidad de Polars se limita al ejecutar el c\u00f3digo de Python, lo que evita que varios subprocesos ejecuten la funci\u00f3n.</p> <p>Todo esto se siente terriblemente limitante, especialmente porque a menudo necesitamos esos <code>lambda</code> en un paso  <code>.groupby()</code>, por ejemplo. Este enfoque a\u00fan es compatible con Polars, pero teniendo en cuenta el c\u00f3digo de bytes Y el precio <code>GIL</code> deben pagarse.</p> <p>Para mitigar esto, Polars implementa una poderosa sintaxis definida no solo en su <code>lazy</code>, sino tambi\u00e9n en su uso <code>eager</code>.</p>"},{"location":"blog/2022/2022-03-16-polars/#expresiones-polares","title":"Expresiones polares\u00b6","text":"<p>En la introducci\u00f3n de la p\u00e1gina anterior, discutimos que el uso de funciones personalizadas de Python eliminaba la paralelizaci\u00f3n y que podemos usar las expresiones de la API diferida para mitigar esto. Echemos un vistazo a lo que eso significa.</p> <p>Comencemos con el conjunto de datos simple del congreso de EE. UU.</p>"},{"location":"blog/2022/2022-03-16-polars/#agregaciones-basicas","title":"Agregaciones b\u00e1sicas\u00b6","text":"<p>Puede combinar f\u00e1cilmente diferentes agregaciones agregando varias expresiones en una lista. No hay un l\u00edmite superior en el n\u00famero de agregaciones que puede hacer y puede hacer cualquier combinaci\u00f3n que desee. En el fragmento a continuaci\u00f3n, hacemos las siguientes agregaciones:</p> <p>Por grupo <code>\"first_name\"</code>:</p> <ul> <li>cuente el n\u00famero de filas en el grupo:<ul> <li>forma abreviada: <code>pl.count(\"party\")</code></li> <li>forma completa: <code>pl.col(\"party\").count()</code></li> </ul> </li> <li>agregue el grupo de valores de g\u00e9nero a una lista:<ul> <li>forma completa: <code>pl.col(\"gender\").list()</code></li> </ul> </li> <li>obtenga el primer valor de la columna <code>\"last_name\"</code> en el grupo:<ul> <li>forma abreviada: <code>pl.primero(\"last_name\")</code></li> <li>forma completa: <code>pl.col(\"last_name\").first()</code></li> </ul> </li> </ul> <p>Adem\u00e1s de la agregaci\u00f3n, clasificamos inmediatamente el resultado y lo limitamos a los 5 principales para que tengamos un buen resumen general.</p>"},{"location":"blog/2022/2022-03-16-polars/#condicionales","title":"Condicionales\u00b6","text":"<p>Ok, eso fue bastante f\u00e1cil, \u00bfverdad? Subamos un nivel. Digamos que queremos saber cu\u00e1ntos delegados de un \"estado\" (<code>state</code>) son administraci\u00f3n \"Democrat\" o \"Republican\". Podr\u00edamos consultarlo directamente en la agregaci\u00f3n sin la necesidad de <code>lambda</code> o arreglar el DataFrame.</p>"},{"location":"blog/2022/2022-03-16-polars/#filtracion","title":"Filtraci\u00f3n\u00b6","text":"<p>Tambi\u00e9n podemos filtrar los grupos. Digamos que queremos calcular una media por grupo, pero no queremos incluir todos los valores de ese grupo y tampoco queremos filtrar las filas del <code>DataFrame</code> (porque necesitamos esas filas para otra agregaci\u00f3n).</p> <p>En el siguiente ejemplo, mostramos c\u00f3mo se puede hacer eso. Tenga en cuenta que podemos hacer funciones de Python para mayor claridad. Estas funciones no nos cuestan nada. Esto se debe a que solo creamos <code>Polars expression</code>, no aplicamos una funci\u00f3n personalizada sobre <code>Series</code> durante el tiempo de ejecuci\u00f3n de la consulta.</p>"},{"location":"blog/2022/2022-03-16-polars/#sorting","title":"Sorting\u00b6","text":"<p>A menudo veo que se ordena un DataFrame con el \u00fanico prop\u00f3sito de ordenar durante la operaci\u00f3n <code>GROUPBY</code>. Digamos que queremos obtener los nombres de los pol\u00edticos m\u00e1s antiguos y m\u00e1s j\u00f3venes (no es que todav\u00eda est\u00e9n vivos) por estado, podr\u00edamos <code>ORDENAR</code> y <code>AGRUPAR</code>.</p>"},{"location":"blog/2022/2022-03-16-polars/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Polars - User Guide</li> <li>Polars - Github</li> </ul>"},{"location":"blog/2022/2022-10-12-causal_impact/","title":"Causal Impact","text":"<p>Importar librer\u00edas</p> <p>En primer lugar, instalemos <code>pycausalimpac</code> para el an\u00e1lisis causal de series de tiempo.</p> In\u00a0[1]: Copied! <pre># Install python version of causal impact\n#!pip install pycausalimpact\n</pre> # Install python version of causal impact #!pip install pycausalimpact <p>Una vez completada la instalaci\u00f3n, podemos importar las bibliotecas.</p> <ul> <li><code>pandas</code>, <code>numpy</code> y <code>datetime</code> se importan para el procesamiento de datos.</li> <li><code>ArmaProcess</code> se importa para la creaci\u00f3n de datos de series temporales sint\u00e9ticas.</li> <li><code>matplotlib</code> y <code>seaborn</code> son para visualizaci\u00f3n.</li> <li><code>CausalImpact</code> es para la estimaci\u00f3n de los efectos del tratamiento de series de tiempo.</li> </ul> <pre># Data processing\nimport pandas as pdm\nimport numpy as np\nfrom datetime import datetime\n\n# Create synthetic time-series data\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Causal impact\nfrom causalimpact import CausalImpact\n</pre> <pre># semilla aleatoria\nnp.random.seed(42)\n\n# Coeficientes autorregresivos\narparams = np.array([.95, .05])\n\n# Coeficientes promedios\nmaparams = np.array([.6, .3])\n\n# Creacion del ARMA proccess\narma_process = ArmaProcess.from_coeffs(arparams, maparams)\n\n# Control Time-series\nX = 10 + arma_process.generate_sample(nsample=500)\n\n# Respuesta del time-series\ny = 2 * X + np.random.normal(size=500)\n\n# Causal impact\ny[300:] += 10\n</pre> <p>Una serie de tiempo generalmente tiene una variable de tiempo que indica la frecuencia de los datos recopilados. Creamos 500 fechas a partir del 1 de enero de 2021 usando la funci\u00f3n pandas <code>date_range</code>, lo que indica que el conjunto de datos tiene datos diarios.</p> <p>Despu\u00e9s de eso, se crea un marco de datos de pandas con la variable de control <code>X</code>, la variable de respuesta es <code>y</code> y las <code>dates</code> como \u00edndice.</p> In\u00a0[4]: Copied! <pre># Create dates\ndates = pd.date_range('2021-01-01', freq='D', periods=500)\n\n# Create dataframe\ndf = pd.DataFrame({'dates': dates, 'y': y, 'X': X}, columns=['dates', 'y', 'X'])\n\n# Set dates as index\ndf.set_index('dates', inplace=True)\n\n# Take a look at the data\ndf.head()\n</pre> # Create dates dates = pd.date_range('2021-01-01', freq='D', periods=500)  # Create dataframe df = pd.DataFrame({'dates': dates, 'y': y, 'X': X}, columns=['dates', 'y', 'X'])  # Set dates as index df.set_index('dates', inplace=True)  # Take a look at the data df.head() Out[4]: y X dates 2021-01-01 21.919606 10.496714 2021-01-02 23.172702 10.631643 2021-01-03 21.278713 11.338640 2021-01-04 26.909878 13.173454 2021-01-05 27.260727 13.955685 In\u00a0[5]: Copied! <pre># Print out the time series start date\nprint(f'The time-series start date is :{df.index.min()}')\n\n# Print out the time series end date\nprint(f'The time-series end date is :{df.index.max()}')\n\n# Print out the intervention start date\nprint(f'The treatment start date is :{df.index[300]}')\n</pre> # Print out the time series start date print(f'The time-series start date is :{df.index.min()}')  # Print out the time series end date print(f'The time-series end date is :{df.index.max()}')  # Print out the intervention start date print(f'The treatment start date is :{df.index[300]}') <pre>The time-series start date is :2021-01-01 00:00:00\nThe time-series end date is :2022-05-15 00:00:00\nThe treatment start date is :2021-10-28 00:00:00\n</pre> <p>A continuaci\u00f3n, visualicemos los datos de la serie temporal.</p> In\u00a0[6]: Copied! <pre># Visualize data using seaborn\nsns.set(rc={'figure.figsize':(12,8)})\nsns.lineplot(x=df.index, y=df['X'])\nsns.lineplot(x=df.index, y=df['y'])\nplt.axvline(x= df.index[300], color='red')\nplt.legend(labels = ['X', 'y'])\nplt.show()\n</pre> # Visualize data using seaborn sns.set(rc={'figure.figsize':(12,8)}) sns.lineplot(x=df.index, y=df['X']) sns.lineplot(x=df.index, y=df['y']) plt.axvline(x= df.index[300], color='red') plt.legend(labels = ['X', 'y']) plt.show() <p>En el gr\u00e1fico, la l\u00ednea azul es la serie temporal de control, la l\u00ednea naranja es la serie temporal de respuesta y la l\u00ednea vertical roja representa la fecha de inicio de la intervenci\u00f3n.</p> <p>Podemos ver que antes de la intervenci\u00f3n, las series temporales de control y respuesta tienen valores similares. Despu\u00e9s de la intervenci\u00f3n, la serie de tiempo de respuesta tiene consistentemente valores m\u00e1s altos que la serie de tiempo de control.</p> <p>El paquete <code>CausalImpact</code> de python requiere las entradas de los per\u00edodos anterior y posterior en un formato de lista. El primer elemento de la lista es el \u00edndice inicial y el \u00faltimo elemento de la lista es el \u00edndice final.</p> <p>La fecha de inicio de la intervenci\u00f3n es <code>2021-10-28</code>, por lo que el per\u00edodo previo finaliza en <code>2021-10-27</code>.</p> In\u00a0[7]: Copied! <pre># Set pre-period\npre_period = [str(df.index.min())[:10], str(df.index[299])[:10]]\n\n# Set post-period\npost_period = [str(df.index[300])[:10], str(df.index.max())[:10]]\n\n# Print out the values\nprint(f'The pre-period is {pre_period}')\nprint(f'The post-period is {post_period}')\n</pre> # Set pre-period pre_period = [str(df.index.min())[:10], str(df.index[299])[:10]]  # Set post-period post_period = [str(df.index[300])[:10], str(df.index.max())[:10]]  # Print out the values print(f'The pre-period is {pre_period}') print(f'The post-period is {post_period}') <pre>The pre-period is ['2021-01-01', '2021-10-27']\nThe post-period is ['2021-10-28', '2022-05-15']\n</pre> In\u00a0[8]: Copied! <pre># Calculate the pre-daily average\npre_daily_avg = df['y'][:300].mean()\n\n# Calculate the post-daily average\npost_daily_avg = df['y'][300:].mean()\n\n# Print out the results\nprint(f'The pre-treatment daily average is {pre_daily_avg}.')\nprint(f'The post-treatment daily average is {post_daily_avg}.')\nprint(f'The raw difference between the pre and the post treatment is {post_daily_avg - pre_daily_avg}.')\n</pre> # Calculate the pre-daily average pre_daily_avg = df['y'][:300].mean()  # Calculate the post-daily average post_daily_avg = df['y'][300:].mean()  # Print out the results print(f'The pre-treatment daily average is {pre_daily_avg}.') print(f'The post-treatment daily average is {post_daily_avg}.') print(f'The raw difference between the pre and the post treatment is {post_daily_avg - pre_daily_avg}.') <pre>The pre-treatment daily average is -1.6403416947312546.\nThe post-treatment daily average is 50.08461262581729.\nThe raw difference between the pre and the post treatment is 51.72495432054855.\n</pre> In\u00a0[9]: Copied! <pre># Causal impact model\nimpact = CausalImpact(data=df, pre_period=pre_period, post_period=post_period)\n\n# Visualization\nimpact.plot()\n\nplt.show()\n</pre> # Causal impact model impact = CausalImpact(data=df, pre_period=pre_period, post_period=post_period)  # Visualization impact.plot()  plt.show() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n</pre> <p>La visualizaci\u00f3n consta de tres gr\u00e1ficos:</p> <ul> <li>El primer gr\u00e1fico traza los valores contrafactuales pronosticados y los valores reales para el per\u00edodo posterior.</li> <li>El segundo gr\u00e1fico representa los efectos puntuales, que son las diferencias entre los valores reales y los previstos. Podemos ver que los valores de los efectos de puntos anteriores al per\u00edodo est\u00e1n alrededor de 0, y los valores de los efectos de puntos posteriores al per\u00edodo est\u00e1n alrededor del impacto real de 10.</li> <li>El tercer gr\u00e1fico traza el efecto acumulativo, que es la suma acumulativa de los efectos de puntos del segundo gr\u00e1fico.</li> </ul> In\u00a0[10]: Copied! <pre># Causal impact summary\nprint(impact.summary())\n</pre> # Causal impact summary print(impact.summary()) <pre>Posterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         40.03 (1.2)        8005.58 (239.36)\n95% CI                    [37.64, 42.33]     [7527.6, 8465.89]\n\nAbsolute effect (s.d.)    10.06 (1.2)        2011.34 (239.36)\n95% CI                    [7.76, 12.45]      [1551.03, 2489.32]\n\nRelative effect (s.d.)    25.12% (2.99%)     25.12% (2.99%)\n95% CI                    [19.37%, 31.09%]   [19.37%, 31.09%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n</pre> <p>Podemos imprimir la versi\u00f3n del informe del resumen usando la opci\u00f3n <code>output='report'</code>.</p> In\u00a0[11]: Copied! <pre># Causal impact report\nprint(impact.summary(output='report'))\n</pre> # Causal impact report print(impact.summary(output='report')) <pre>Analysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had\nan average value of approx. 50.08. By contrast, in the absence of an\nintervention, we would have expected an average response of 40.03.\nThe 95% interval of this counterfactual prediction is [37.64, 42.33].\nSubtracting this prediction from the observed response yields\nan estimate of the causal effect the intervention had on the\nresponse variable. This effect is 10.06 with a 95% interval of\n[7.76, 12.45]. For a discussion of the significance of this effect,\nsee below.\n\n\nSumming up the individual data points during the post-intervention\nperiod (which can only sometimes be meaningfully interpreted), the\nresponse variable had an overall value of 10016.92.\nBy contrast, had the intervention not taken place, we would have expected\na sum of 8005.58. The 95% interval of this prediction is [7527.6, 8465.89].\n\n\nThe above results are given in terms of absolute numbers. In relative\nterms, the response variable showed an increase of +25.12%. The 95%\ninterval of this percentage is [19.37%, 31.09%].\n\n\nThis means that the positive effect observed during the intervention\nperiod is statistically significant and unlikely to be due to random\nfluctuations. It should be noted, however, that the question of whether\nthis increase also bears substantive significance can only be answered\nby comparing the absolute effect (10.06) to the original goal\nof the underlying intervention.\n\n\nThe probability of obtaining this effect by chance is very small\n(Bayesian one-sided tail-area probability p = 0.0).\nThis means the causal effect can be considered statistically\nsignificant.\n</pre> In\u00a0[12]: Copied! <pre># Causal impact model without prior level sd\nimpact_no_prior_level_sd = CausalImpact(df, pre_period, post_period, prior_level_sd=None)\n\n# Plot the results\nimpact_no_prior_level_sd.plot()\n</pre> # Causal impact model without prior level sd impact_no_prior_level_sd = CausalImpact(df, pre_period, post_period, prior_level_sd=None)  # Plot the results impact_no_prior_level_sd.plot() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize, prior_level_sd. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n</pre> <p>Podemos ver que los valores de estimaci\u00f3n puntual son similares, pero las desviaciones est\u00e1ndar son m\u00e1s peque\u00f1as para la estimaci\u00f3n.</p> In\u00a0[13]: Copied! <pre># Print out the summary\nprint(impact_no_prior_level_sd.summary())\n</pre> # Print out the summary print(impact_no_prior_level_sd.summary()) <pre>Posterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         39.84 (0.09)       7967.47 (18.41)\n95% CI                    [39.65, 40.01]     [7929.85, 8001.99]\n\nAbsolute effect (s.d.)    10.25 (0.09)       2049.45 (18.41)\n95% CI                    [10.07, 10.44]     [2014.93, 2087.08]\n\nRelative effect (s.d.)    25.72% (0.23%)     25.72% (0.23%)\n95% CI                    [25.29%, 26.19%]   [25.29%, 26.19%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n</pre>"},{"location":"blog/2022/2022-10-12-causal_impact/#causal-impact","title":"Causal Impact\u00b6","text":""},{"location":"blog/2022/2022-10-12-causal_impact/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>El paquete <code>CausalImpact</code> creado por Google estima el impacto de una intervenci\u00f3n en una serie temporal. Por ejemplo, \u00bfc\u00f3mo afecta una nueva funci\u00f3n en una aplicaci\u00f3n el tiempo de los usuarios en la aplicaci\u00f3n?</p> <p>En este tutorial, hablaremos sobre c\u00f3mo usar el paquete de Python <code>CausalImpact</code> para hacer inferencias causales de series de tiempo. Aprender\u00e1s:</p> <ul> <li>\u00bfC\u00f3mo establecer los per\u00edodos previo y posterior para el an\u00e1lisis de impacto causal?</li> <li>\u00bfC\u00f3mo realizar inferencias causales sobre datos de series temporales?</li> <li>\u00bfC\u00f3mo resumir los resultados del an\u00e1lisis de causalidad y crear un informe?</li> <li>\u00bfCu\u00e1les son las diferencias entre los paquetes python y R para <code>CausalImpact</code>?</li> </ul>"},{"location":"blog/2022/2022-10-12-causal_impact/#crear-el-conjunto-de-datos","title":"Crear el conjunto de datos\u00b6","text":"<p>Crearemos un conjunto de datos de series de tiempo sint\u00e9tico para el an\u00e1lisis de impacto causal. El beneficio de usar un conjunto de datos sint\u00e9tico es que podemos validar la precisi\u00f3n de los resultados del modelo.</p> <p>El paquete CausalImpact requiere dos tipos de series temporales:</p> <ul> <li>Una serie temporal de respuesta que se ve directamente afectada por la intervenci\u00f3n.</li> <li>Y una o m\u00e1s series temporales de control que no se ven afectadas por la intervenci\u00f3n.</li> </ul> <p>La idea es construir un modelo de serie de tiempo para predecir el resultado contraf\u00e1ctico. En otras palabras, el modelo utilizar\u00e1 la serie temporal de control para predecir cu\u00e1l habr\u00eda sido el resultado de la serie temporal de respuesta si no hubiera habido intervenci\u00f3n.</p> <p>En este ejemplo, creamos una variable de serie temporal de respuesta y una variable de serie temporal de control.</p> <ul> <li>Para que el conjunto de datos sea reproducible, se establece una semilla aleatoria al comienzo del c\u00f3digo.</li> <li>Luego se crea un proceso de promedio m\u00f3vil autorregresivo (ARMA). La parte autorregresiva (AR) tiene dos coeficientes 0,95 y 0,05, y la parte de media m\u00f3vil (MA) tiene dos coeficientes 0,6 y 0,3.</li> <li>Despu\u00e9s de crear el proceso de media m\u00f3vil autorregresiva (ARMA), se generan 500 muestras a partir del proceso.</li> <li>La variable de serie temporal de control <code>X</code> se crea a\u00f1adiendo un valor constante de 10 a los valores generados.</li> <li>La variable de serie temporal de respuesta <code>y</code> es una funci\u00f3n de la variable de serie temporal de control <code>X</code>. Es igual a 2 veces <code>X</code> m\u00e1s un valor aleatorio.</li> <li>La intervenci\u00f3n ocurre en el \u00edndice de 300, y el verdadero impacto causal es 10.</li> </ul>"},{"location":"blog/2022/2022-10-12-causal_impact/#periodos-anteriores-y-posteriores","title":"Per\u00edodos anteriores y posteriores\u00b6","text":"<p>Estableceremos los periodos de pre y post intervenci\u00f3n. Usando <code>df.index</code>, podemos ver que la fecha de inicio de la serie temporal es <code>2021-01-01</code>, la fecha de finalizaci\u00f3n de la serie temporal es <code>2022-05-15</code> y la fecha de inicio del tratamiento es <code>2021- 10-28</code>.</p>"},{"location":"blog/2022/2022-10-12-causal_impact/#diferencias-sin-procesar","title":"Diferencias sin procesar\u00b6","text":"<p>Calcularemos la diferencia bruta entre los per\u00edodos previo y posterior.</p> <p>Podemos ver que el promedio diario previo al tratamiento es -1,64, el promedio diario posterior al tratamiento es 50,08 y la diferencia bruta entre el tratamiento previo y posterior es 51,7, que es mucho mayor que el verdadero impacto causal de 10.</p> <p>Sin an\u00e1lisis de causalidad, sobreestimaremos el impacto causal.</p>"},{"location":"blog/2022/2022-10-12-causal_impact/#causal-impact-en-series-de-tiempo","title":"Causal Impact en series de tiempo\u00b6","text":"<p>ejecutaremos el an\u00e1lisis de impacto causal sobre la serie temporal.</p> <p>El an\u00e1lisis de causalidad tiene dos supuestos:</p> <ul> <li>Supuesto 1: Hay una o m\u00e1s series temporales de control que est\u00e1n altamente correlacionadas con la variable de respuesta, pero que no se ven afectadas por la intervenci\u00f3n. La violaci\u00f3n de esta suposici\u00f3n puede dar lugar a conclusiones err\u00f3neas sobre la existencia, la direcci\u00f3n o la magnitud del efecto del tratamiento.</li> <li>Supuesto 2: La correlaci\u00f3n entre el control y la serie temporal de respuesta es la misma para antes y despu\u00e9s de la intervenci\u00f3n.</li> </ul> <p>Los datos de series de tiempo sint\u00e9ticos que creamos satisfacen las dos suposiciones.</p> <p>El paquete <code>CausalImpact</code> de python tiene una funci\u00f3n llamada <code>CausalImpact</code> que implementa un modelo de serie de tiempo estructural bayesiano (BSTS) en el backend. Tiene tres entradas requeridas:</p> <ul> <li><code>data</code> toma el nombre del dataframe de python.</li> <li><code>pre_period</code> toma los valores de \u00edndice inicial y final para el per\u00edodo previo a la intervenci\u00f3n.</li> <li><code>post_period</code> toma los valores de \u00edndice inicial y final para el per\u00edodo posterior a la intervenci\u00f3n.</li> </ul> <p>Despu\u00e9s de guardar el objeto de salida en una variable llamada <code>impact</code>, podemos ejecutar <code>impact.plot()</code> para visualizar los resultados.</p>"},{"location":"blog/2022/2022-10-12-causal_impact/#resumen-causal-impact","title":"Resumen (Causal Impact)\u00b6","text":"<p>Resumiremos el impacto causal de la intervenci\u00f3n para la serie temporal.</p> <p>El resumen de <code>impact.summary()</code> nos dice que:</p> <ul> <li>El promedio posterior a la intervenci\u00f3n real es 50,08 y el promedio posterior a la intervenci\u00f3n pronosticado es 40,3.</li> <li>El efecto causal absoluto es 10,06, que est\u00e1 muy cerca del verdadero impacto de 10 y mucho mejor que la diferencia bruta de 51,7.</li> <li>El efecto causal relativo es del 25,12%.</li> <li>La probabilidad posterior de un efecto causal es del 100%, lo que demuestra que el modelo tiene mucha confianza en que existe el impacto causal.</li> </ul>"},{"location":"blog/2022/2022-10-12-causal_impact/#causalimpact-python-vs-r","title":"CausalImpact:  Python vs R\u00b6","text":"<p>Hablaremos sobre las diferencias del paquete <code>CausalImpact</code> de Google entre Python y R.</p> <p>El paquete python fue portado desde el paquete R, por lo que la mayor\u00eda de las veces los dos paquetes producen resultados similares, pero a veces producen resultados diferentes.</p> <p>Las diferencias son causadas por suposiciones para inicializaciones previas, el proceso de optimizaci\u00f3n y los algoritmos de implementaci\u00f3n.</p> <p>La documentaci\u00f3n del paquete <code>pycausalimpact</code> recomienda encarecidamente establecer <code>prior_level_sd</code> en <code>Ninguno</code>, lo que permitir\u00e1 que <code>statsmodel</code> realice la optimizaci\u00f3n para el componente anterior en el nivel local.</p> <p>En base a esta sugerencia, se crea una versi\u00f3n con la opci\u00f3n <code>prior_level_sd=None</code>.</p>"},{"location":"blog/2022/2022-10-12-causal_impact/#ajuste-de-hiperparametros","title":"Ajuste de hiperpar\u00e1metros\u00b6","text":"<p>Para aprender a ajustar los hiperpar\u00e1metros del modelo de impacto causal de series temporales con el paquete CausalImpact de python, consulte eltutorial Hyperparameter Tuning for Time Series Causal Impact Analysis in Python</p>"},{"location":"blog/2022/2022-10-12-causal_impact/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Inferring causal impact using Bayesian structural time-series models.</li> <li>Time Series Causal Impact Analysis in Python | Machine Learning.</li> <li>Inferring the effect of an event using CausalImpact by Kay Brodersen.</li> </ul>"},{"location":"blog/2022/2022-10-12-implicit/","title":"Collaborative Filtering","text":"<p>Este gr\u00e1fico ilustra c\u00f3mo funciona el filtrado colaborativo basado en elementos mediante un ejemplo simplificado.</p> <ul> <li>A la Sra. Blond le gustan las manzanas, las sand\u00edas y las pi\u00f1as. A la Sra. Black le gusta la sand\u00eda y la pi\u00f1a. A la Sra. P\u00farpura le gustan las sand\u00edas y las uvas.</li> <li>Debido a que a la Sra. Black y la Sra. Purple les gustan tanto las sand\u00edas como las pi\u00f1as, consideramos que las sand\u00edas y las pi\u00f1as son art\u00edculos similares.</li> <li>Dado que a la Sra. P\u00farpura le gustan las sand\u00edas y a\u00fan no ha estado expuesta a la pi\u00f1a, el sistema de recomendaci\u00f3n recomienda la pi\u00f1a a la Sra. P\u00farpura.</li> </ul> In\u00a0[1]: Copied! <pre># Data processing\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\n\n# Visualization\nimport seaborn as sns\n\n# Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport warnings\nwarnings.simplefilter('ignore')\n</pre> # Data processing import pandas as pd import numpy as np import scipy.stats  # Visualization import seaborn as sns  # Similarity from sklearn.metrics.pairwise import cosine_similarity  import warnings warnings.simplefilter('ignore')  In\u00a0[2]: Copied! <pre># Read in data\nratings=pd.read_csv('data/ml-latest-small/ratings.csv')\n\n# Take a look at the data\nratings.head()\n</pre> # Read in data ratings=pd.read_csv('data/ml-latest-small/ratings.csv')  # Take a look at the data ratings.head() Out[2]: userId movieId rating timestamp 0 1 1 4.0 964982703 1 1 3 4.0 964981247 2 1 6 4.0 964982224 3 1 47 5.0 964983815 4 1 50 5.0 964982931 <p>Hay cuatro columnas en el conjunto de datos de calificaciones:</p> <ul> <li>userID</li> <li>movieID</li> <li>rating</li> <li>timestamp</li> </ul> <p>El conjunto de datos tiene m\u00e1s de 100 000 registros y no falta ning\u00fan dato.</p> In\u00a0[3]: Copied! <pre># Get the dataset information\nratings.info()\n</pre> # Get the dataset information ratings.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100836 entries, 0 to 100835\nData columns (total 4 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   userId     100836 non-null  int64  \n 1   movieId    100836 non-null  int64  \n 2   rating     100836 non-null  float64\n 3   timestamp  100836 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 3.1 MB\n</pre> <p>Las calificaciones de 100k son de 610 usuarios en 9724 pel\u00edculas. La calificaci\u00f3n tiene diez valores \u00fanicos de 0.5 a 5.</p> In\u00a0[4]: Copied! <pre># Number of users\nprint('The ratings dataset has', ratings['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', ratings['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', ratings['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(ratings['rating'].unique()))\n</pre> # Number of users print('The ratings dataset has', ratings['userId'].nunique(), 'unique users')  # Number of movies print('The ratings dataset has', ratings['movieId'].nunique(), 'unique movies')  # Number of ratings print('The ratings dataset has', ratings['rating'].nunique(), 'unique ratings')  # List of unique ratings print('The unique ratings are', sorted(ratings['rating'].unique())) <pre>The ratings dataset has 610 unique users\nThe ratings dataset has 9724 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n</pre> <p>A continuaci\u00f3n, leamos los datos de las pel\u00edculas para obtener los nombres de las pel\u00edculas (<code>movies.csv</code>).</p> <p>El conjunto de datos de pel\u00edculas tiene:</p> <ul> <li>movieId</li> <li>title</li> <li>genres</li> </ul> In\u00a0[5]: Copied! <pre># Read in data\nmovies = pd.read_csv('data/ml-latest-small/movies.csv')\n\n# Take a look at the data\nmovies.head()\n</pre> # Read in data movies = pd.read_csv('data/ml-latest-small/movies.csv')  # Take a look at the data movies.head() Out[5]: movieId title genres 0 1 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 1 2 Jumanji (1995) Adventure|Children|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama|Romance 4 5 Father of the Bride Part II (1995) Comedy <p>Usando <code>movieID</code> como clave coincidente, agregamos informaci\u00f3n de la pel\u00edcula al conjunto de datos de calificaci\u00f3n y lo llamamos <code>df</code>. \u00a1As\u00ed que ahora tenemos el t\u00edtulo de la pel\u00edcula y la calificaci\u00f3n de la pel\u00edcula en el mismo conjunto de datos!</p> In\u00a0[6]: Copied! <pre># Merge ratings and movies datasets\ndf = pd.merge(ratings, movies, on='movieId', how='inner')\n\n# Take a look at the data\ndf.head()\n</pre> # Merge ratings and movies datasets df = pd.merge(ratings, movies, on='movieId', how='inner')  # Take a look at the data df.head() Out[6]: userId movieId rating timestamp title genres 0 1 1 4.0 964982703 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 1 5 1 4.0 847434962 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 2 7 1 4.5 1106635946 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 3 15 1 2.5 1510577970 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 4 17 1 4.5 1305696483 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy In\u00a0[7]: Copied! <pre># Aggregate by movie\nagg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),\n                                                number_of_ratings = ('rating', 'count')).reset_index()\n\n# Keep the movies with over 100 ratings\nagg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']&gt;100]\nagg_ratings_GT100.info()\n</pre> # Aggregate by movie agg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),                                                 number_of_ratings = ('rating', 'count')).reset_index()  # Keep the movies with over 100 ratings agg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']&gt;100] agg_ratings_GT100.info()   <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 134 entries, 74 to 9615\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              134 non-null    object \n 1   mean_rating        134 non-null    float64\n 2   number_of_ratings  134 non-null    int64  \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 4.2+ KB\n</pre> <p>Veamos cu\u00e1les son las pel\u00edculas m\u00e1s populares y sus calificaciones.</p> In\u00a0[8]: Copied! <pre># Check popular movies\nagg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head()\n</pre> # Check popular movies agg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head() Out[8]: title mean_rating number_of_ratings 3158 Forrest Gump (1994) 4.164134 329 7593 Shawshank Redemption, The (1994) 4.429022 317 6865 Pulp Fiction (1994) 4.197068 307 7680 Silence of the Lambs, The (1991) 4.161290 279 5512 Matrix, The (1999) 4.192446 278 <p>A continuaci\u00f3n, usemos un  <code>jointplot</code> para verificar la correlaci\u00f3n entre la calificaci\u00f3n promedio y el n\u00famero de calificaciones.</p> <p>Podemos ver una tendencia ascendente en el diagrama de dispersi\u00f3n, que muestra que las pel\u00edculas populares obtienen calificaciones m\u00e1s altas.</p> <p>La distribuci\u00f3n de calificaci\u00f3n promedio muestra que la mayor\u00eda de las pel\u00edculas en el conjunto de datos tienen una calificaci\u00f3n promedio de alrededor de 4.</p> <p>El n\u00famero de distribuci\u00f3n de calificaciones muestra que la mayor\u00eda de las pel\u00edculas tienen menos de 150 calificaciones.</p> In\u00a0[9]: Copied! <pre># Visulization\nsns.jointplot(\n    x='mean_rating',\n    y='number_of_ratings',\n    data=agg_ratings_GT100,\n    color='gray'\n)\n</pre> # Visulization sns.jointplot(     x='mean_rating',     y='number_of_ratings',     data=agg_ratings_GT100,     color='gray' ) Out[9]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x263dc2b6070&gt;</pre> <p>Para mantener solo las 134 pel\u00edculas con m\u00e1s de 100 calificaciones, debemos unir la pel\u00edcula con el dataframe del nivel de calificaci\u00f3n del usuario.</p> <p><code>how='inner'</code> y <code>on='title'</code> aseguran que solo se incluyan las pel\u00edculas con m\u00e1s de 100 calificaciones.</p> In\u00a0[10]: Copied! <pre># Merge data\ndf_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner')\ndf_GT100.info()\n</pre> # Merge data df_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner') df_GT100.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 19788 entries, 0 to 19787\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   userId     19788 non-null  int64  \n 1   movieId    19788 non-null  int64  \n 2   rating     19788 non-null  float64\n 3   timestamp  19788 non-null  int64  \n 4   title      19788 non-null  object \n 5   genres     19788 non-null  object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 1.1+ MB\n</pre> <p>Despu\u00e9s de filtrar las pel\u00edculas con m\u00e1s de 100 calificaciones, tenemos 597 usuarios que calificaron 134 pel\u00edculas.</p> In\u00a0[11]: Copied! <pre># Number of users\nprint('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(df_GT100['rating'].unique()))\n</pre> # Number of users print('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')  # Number of movies print('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')  # Number of ratings print('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')  # List of unique ratings print('The unique ratings are', sorted(df_GT100['rating'].unique())) <pre>The ratings dataset has 597 unique users\nThe ratings dataset has 134 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n</pre> In\u00a0[12]: Copied! <pre># Create user-item matrix\nmatrix = df_GT100.pivot_table(index='userId', columns='title', values='rating')\nmatrix.head()\n</pre> # Create user-item matrix matrix = df_GT100.pivot_table(index='userId', columns='title', values='rating') matrix.head() Out[12]: title 2001: A Space Odyssey (1968) Ace Ventura: Pet Detective (1994) Aladdin (1992) Alien (1979) Aliens (1986) Amelie (Fabuleux destin d'Am\u00e9lie Poulain, Le) (2001) American Beauty (1999) American History X (1998) American Pie (1999) Apocalypse Now (1979) ... True Lies (1994) Truman Show, The (1998) Twelve Monkeys (a.k.a. 12 Monkeys) (1995) Twister (1996) Up (2009) Usual Suspects, The (1995) WALL\u00b7E (2008) Waterworld (1995) Willy Wonka &amp; the Chocolate Factory (1971) X-Men (2000) userId 1 NaN NaN NaN 4.0 NaN NaN 5.0 5.0 NaN 4.0 ... NaN NaN NaN 3.0 NaN 5.0 NaN NaN 5.0 5.0 2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 NaN NaN 4.0 NaN NaN NaN 5.0 NaN NaN NaN ... NaN NaN 2.0 NaN NaN NaN NaN NaN 4.0 NaN 5 NaN 3.0 4.0 NaN NaN NaN NaN NaN NaN NaN ... 2.0 NaN NaN NaN NaN 4.0 NaN NaN NaN NaN <p>5 rows \u00d7 134 columns</p> In\u00a0[13]: Copied! <pre># Normalize user-item matrix\nmatrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')\nmatrix_norm.head()\n</pre> # Normalize user-item matrix matrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows') matrix_norm.head() Out[13]: title 2001: A Space Odyssey (1968) Ace Ventura: Pet Detective (1994) Aladdin (1992) Alien (1979) Aliens (1986) Amelie (Fabuleux destin d'Am\u00e9lie Poulain, Le) (2001) American Beauty (1999) American History X (1998) American Pie (1999) Apocalypse Now (1979) ... True Lies (1994) Truman Show, The (1998) Twelve Monkeys (a.k.a. 12 Monkeys) (1995) Twister (1996) Up (2009) Usual Suspects, The (1995) WALL\u00b7E (2008) Waterworld (1995) Willy Wonka &amp; the Chocolate Factory (1971) X-Men (2000) userId 1 NaN NaN NaN -0.392857 NaN NaN 0.607143 0.607143 NaN -0.392857 ... NaN NaN NaN -1.392857 NaN 0.607143 NaN NaN 0.607143 0.607143 2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 NaN NaN 0.617647 NaN NaN NaN 1.617647 NaN NaN NaN ... NaN NaN -1.382353 NaN NaN NaN NaN NaN 0.617647 NaN 5 NaN -0.461538 0.538462 NaN NaN NaN NaN NaN NaN NaN ... -1.461538 NaN NaN NaN NaN 0.538462 NaN NaN NaN NaN <p>5 rows \u00d7 134 columns</p> In\u00a0[14]: Copied! <pre># User similarity matrix using Pearson correlation\nuser_similarity = matrix_norm.T.corr()\nuser_similarity.head()\n</pre> # User similarity matrix using Pearson correlation user_similarity = matrix_norm.T.corr() user_similarity.head() Out[14]: userId 1 2 3 4 5 6 7 8 9 10 ... 601 602 603 604 605 606 607 608 609 610 userId 1 1.000000 NaN NaN 0.391797 0.180151 -0.439941 -0.029894 0.464277 1.0 -0.037987 ... 0.091574 0.254514 0.101482 -0.500000 0.780020 0.303854 -0.012077 0.242309 -0.175412 0.071553 2 NaN 1.0 NaN NaN NaN NaN NaN NaN NaN 1.000000 ... -0.583333 NaN -1.000000 NaN NaN 0.583333 NaN -0.229416 NaN 0.765641 3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 0.391797 NaN NaN 1.000000 -0.394823 0.421927 0.704669 0.055442 NaN 0.360399 ... -0.239325 0.562500 0.162301 -0.158114 0.905134 0.021898 -0.020659 -0.286872 NaN -0.050868 5 0.180151 NaN NaN -0.394823 1.000000 -0.006888 0.328889 0.030168 NaN -0.777714 ... 0.000000 0.231642 0.131108 0.068621 -0.245026 0.377341 0.228218 0.263139 0.384111 0.040582 <p>5 rows \u00d7 597 columns</p> <p>Aquellos que est\u00e9n interesados en usar la similitud del coseno pueden consultar este c\u00f3digo. Dado que <code>cosine_similarity</code> no toma valores perdidos, necesitamos imputar los valores perdidos con 0 antes del c\u00e1lculo.</p> In\u00a0[15]: Copied! <pre># User similarity matrix using cosine similarity\nuser_similarity_cosine = cosine_similarity(matrix_norm.fillna(0))\nuser_similarity_cosine\n</pre> # User similarity matrix using cosine similarity user_similarity_cosine = cosine_similarity(matrix_norm.fillna(0)) user_similarity_cosine Out[15]: <pre>array([[ 1.        ,  0.        ,  0.        , ...,  0.14893867,\n        -0.06003146,  0.04528224],\n       [ 0.        ,  1.        ,  0.        , ..., -0.04485403,\n        -0.25197632,  0.18886414],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.14893867, -0.04485403,  0.        , ...,  1.        ,\n         0.14734568,  0.07931015],\n       [-0.06003146, -0.25197632,  0.        , ...,  0.14734568,\n         1.        , -0.14276787],\n       [ 0.04528224,  0.18886414,  0.        , ...,  0.07931015,\n        -0.14276787,  1.        ]])</pre> <p>Ahora usemos el ID de usuario 1 como ejemplo para ilustrar c\u00f3mo encontrar usuarios similares.</p> <p>Primero debemos excluir el ID de usuario 1 de la lista de usuarios similares y decidir el n\u00famero de usuarios similares.</p> In\u00a0[16]: Copied! <pre># Pick a user ID\npicked_userid = 1\n\n# Remove picked user ID from the candidate list\nuser_similarity.drop(index=picked_userid, inplace=True)\n\n# Take a look at the data\nuser_similarity.head()\n</pre> # Pick a user ID picked_userid = 1  # Remove picked user ID from the candidate list user_similarity.drop(index=picked_userid, inplace=True)  # Take a look at the data user_similarity.head() Out[16]: userId 1 2 3 4 5 6 7 8 9 10 ... 601 602 603 604 605 606 607 608 609 610 userId 2 NaN 1.0 NaN NaN NaN NaN NaN NaN NaN 1.000000 ... -0.583333 NaN -1.000000 NaN NaN 0.583333 NaN -0.229416 NaN 0.765641 3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 0.391797 NaN NaN 1.000000 -0.394823 0.421927 0.704669 0.055442 NaN 0.360399 ... -0.239325 0.562500 0.162301 -0.158114 0.905134 0.021898 -0.020659 -0.286872 NaN -0.050868 5 0.180151 NaN NaN -0.394823 1.000000 -0.006888 0.328889 0.030168 NaN -0.777714 ... 0.000000 0.231642 0.131108 0.068621 -0.245026 0.377341 0.228218 0.263139 0.384111 0.040582 6 -0.439941 NaN NaN 0.421927 -0.006888 1.000000 0.000000 -0.127385 NaN 0.957427 ... -0.292770 -0.030599 -0.123983 -0.176327 0.063861 -0.468008 0.541386 -0.337129 0.158255 -0.030567 <p>5 rows \u00d7 597 columns</p> <p>En la matriz de similitud del usuario, los valores var\u00edan de -1 a 1, donde -1 significa la preferencia de pel\u00edcula opuesta y 1 significa la misma preferencia de pel\u00edcula.</p> <p><code>n = 10</code> significa que nos gustar\u00eda elegir los 10 usuarios m\u00e1s similares para el ID de usuario 1.</p> <p>El filtrado colaborativo basado en usuarios hace recomendaciones basadas en usuarios con gustos similares, por lo que debemos establecer un umbral positivo. Aqu\u00ed configuramos <code>user_similarity_threshold</code> en 0,3, lo que significa que un usuario debe tener un coeficiente de correlaci\u00f3n de Pearson de al menos 0,3 para ser considerado como un usuario similar.</p> <p>Despu\u00e9s de establecer la cantidad de usuarios similares y el umbral de similitud, clasificamos el valor de similitud del usuario del m\u00e1s alto al m\u00e1s bajo, luego imprimimos la ID de los usuarios m\u00e1s similares y el valor de correlaci\u00f3n de Pearson.</p> In\u00a0[17]: Copied! <pre># Number of similar users\nn = 10\n\n# User similarity threashold\nuser_similarity_threshold = 0.3\n\n# Get top n similar users\nsimilar_users = user_similarity[user_similarity[picked_userid]&gt;user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]\n\n# Print out top n similar users\nprint(f'The similar users for user {picked_userid} are', similar_users)\n</pre> # Number of similar users n = 10  # User similarity threashold user_similarity_threshold = 0.3  # Get top n similar users similar_users = user_similarity[user_similarity[picked_userid]&gt;user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]  # Print out top n similar users print(f'The similar users for user {picked_userid} are', similar_users) <pre>The similar users for user 1 are userId\n108    1.000000\n9      1.000000\n550    1.000000\n598    1.000000\n502    1.000000\n401    0.942809\n511    0.925820\n366    0.872872\n154    0.866025\n595    0.866025\nName: 1, dtype: float64\n</pre> In\u00a0[18]: Copied! <pre># Movies that the target user has watched\npicked_userid_watched = matrix_norm[matrix_norm.index == picked_userid].dropna(axis=1, how='all')\npicked_userid_watched\n</pre> # Movies that the target user has watched picked_userid_watched = matrix_norm[matrix_norm.index == picked_userid].dropna(axis=1, how='all') picked_userid_watched Out[18]: title Alien (1979) American Beauty (1999) American History X (1998) Apocalypse Now (1979) Back to the Future (1985) Batman (1989) Big Lebowski, The (1998) Braveheart (1995) Clear and Present Danger (1994) Clerks (1994) ... Star Wars: Episode IV - A New Hope (1977) Star Wars: Episode V - The Empire Strikes Back (1980) Star Wars: Episode VI - Return of the Jedi (1983) Stargate (1994) Terminator, The (1984) Toy Story (1995) Twister (1996) Usual Suspects, The (1995) Willy Wonka &amp; the Chocolate Factory (1971) X-Men (2000) userId 1 -0.392857 0.607143 0.607143 -0.392857 0.607143 -0.392857 0.607143 -0.392857 -0.392857 -1.392857 ... 0.607143 0.607143 0.607143 -1.392857 0.607143 -0.392857 -1.392857 0.607143 0.607143 0.607143 <p>1 rows \u00d7 56 columns</p> <p>Para mantener solo las pel\u00edculas de los usuarios similares, mantenemos los ID de usuario en las 10 listas de usuarios similares principales y eliminamos la pel\u00edcula con todos los valores faltantes. Todo valor faltante para una pel\u00edcula significa que ninguno de los usuarios similares ha visto la pel\u00edcula.</p> In\u00a0[19]: Copied! <pre># Movies that similar users watched. Remove movies that none of the similar users have watched\nsimilar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all')\nsimilar_user_movies\n</pre> # Movies that similar users watched. Remove movies that none of the similar users have watched similar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all') similar_user_movies Out[19]: title Aladdin (1992) Alien (1979) Amelie (Fabuleux destin d'Am\u00e9lie Poulain, Le) (2001) Back to the Future (1985) Batman Begins (2005) Beautiful Mind, A (2001) Beauty and the Beast (1991) Blade Runner (1982) Bourne Identity, The (2002) Braveheart (1995) ... Shrek (2001) Silence of the Lambs, The (1991) Spider-Man (2002) Star Wars: Episode I - The Phantom Menace (1999) Terminator 2: Judgment Day (1991) Titanic (1997) Toy Story (1995) Up (2009) Usual Suspects, The (1995) WALL\u00b7E (2008) userId 9 NaN NaN NaN 0.333333 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 108 NaN NaN 0.466667 0.466667 NaN 0.466667 NaN 0.466667 NaN NaN ... NaN NaN 0.466667 NaN NaN -0.533333 NaN NaN NaN NaN 154 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN 0.214286 NaN NaN 366 NaN NaN NaN NaN -0.205882 NaN NaN NaN NaN -0.205882 ... NaN NaN NaN NaN -0.205882 NaN NaN NaN NaN NaN 401 -0.382353 NaN NaN NaN NaN NaN -0.382353 NaN NaN NaN ... 0.117647 NaN NaN NaN NaN NaN 0.117647 0.617647 NaN 0.617647 502 NaN -0.375 NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 511 NaN NaN -0.653846 NaN NaN NaN NaN NaN NaN NaN ... NaN NaN -1.153846 -0.653846 NaN NaN NaN -0.153846 NaN NaN 550 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN -0.277778 0.222222 NaN -0.277778 595 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN -0.333333 NaN NaN NaN NaN 0.666667 NaN 598 NaN NaN NaN NaN NaN NaN NaN NaN 0.888889 NaN ... -2.111111 -2.611111 NaN NaN NaN NaN NaN NaN NaN NaN <p>10 rows \u00d7 62 columns</p> <p>A continuaci\u00f3n, eliminaremos las pel\u00edculas que el usuario ID 1 vio de la lista de pel\u00edculas de usuarios similares. <code>errors='ignore'</code> elimina columnas si existen sin dar un mensaje de error.</p> In\u00a0[20]: Copied! <pre># Remove the watched movie from the movie list\nsimilar_user_movies.drop(picked_userid_watched.columns,axis=1, inplace=True, errors='ignore')\n\n# Take a look at the data\nsimilar_user_movies\n</pre> # Remove the watched movie from the movie list similar_user_movies.drop(picked_userid_watched.columns,axis=1, inplace=True, errors='ignore')  # Take a look at the data similar_user_movies Out[20]: title Aladdin (1992) Amelie (Fabuleux destin d'Am\u00e9lie Poulain, Le) (2001) Batman Begins (2005) Beautiful Mind, A (2001) Beauty and the Beast (1991) Blade Runner (1982) Bourne Identity, The (2002) Breakfast Club, The (1985) Catch Me If You Can (2002) Dark Knight, The (2008) ... Monsters, Inc. (2001) Ocean's Eleven (2001) Pirates of the Caribbean: The Curse of the Black Pearl (2003) Shawshank Redemption, The (1994) Shrek (2001) Spider-Man (2002) Terminator 2: Judgment Day (1991) Titanic (1997) Up (2009) WALL\u00b7E (2008) userId 9 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 108 NaN 0.466667 NaN 0.466667 NaN 0.466667 NaN -0.533333 0.466667 NaN ... NaN NaN NaN NaN NaN 0.466667 NaN -0.533333 NaN NaN 154 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0.214286 NaN 366 NaN NaN -0.205882 NaN NaN NaN NaN NaN NaN -0.205882 ... NaN NaN -0.205882 NaN NaN NaN -0.205882 NaN NaN NaN 401 -0.382353 NaN NaN NaN -0.382353 NaN NaN NaN NaN NaN ... 0.117647 NaN 0.117647 NaN 0.117647 NaN NaN NaN 0.617647 0.617647 502 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN 0.125000 NaN NaN NaN NaN NaN NaN 511 NaN -0.653846 NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN 0.346154 NaN -1.153846 NaN NaN -0.153846 NaN 550 NaN NaN NaN NaN NaN NaN NaN NaN -0.277778 -0.277778 ... NaN NaN NaN 0.222222 NaN NaN NaN NaN 0.222222 -0.277778 595 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 598 NaN NaN NaN NaN NaN NaN 0.888889 NaN NaN NaN ... NaN 0.888889 NaN NaN -2.111111 NaN NaN NaN NaN NaN <p>10 rows \u00d7 38 columns</p> In\u00a0[21]: Copied! <pre># A dictionary to store item scores\nitem_score = {}\n\n# Loop through items\nfor i in similar_user_movies.columns:\n  # Get the ratings for movie i\n  movie_rating = similar_user_movies[i]\n  # Create a variable to store the score\n  total = 0\n  # Create a variable to store the number of scores\n  count = 0\n  # Loop through similar users\n  for u in similar_users.index:\n    # If the movie has rating\n    if pd.isna(movie_rating[u]) == False:\n      # Score is the sum of user similarity score multiply by the movie rating\n      score = similar_users[u] * movie_rating[u]\n      # Add the score to the total score for the movie so far\n      total += score\n      # Add 1 to the count\n      count +=1\n  # Get the average score for the item\n  item_score[i] = total / count\n\n# Convert dictionary to pandas dataframe\nitem_score = pd.DataFrame(item_score.items(), columns=['movie', 'movie_score'])\n    \n# Sort the movies by score\nranked_item_score = item_score.sort_values(by='movie_score', ascending=False)\n\n# Select top m movies\nm = 10\nranked_item_score.head(m)\n</pre> # A dictionary to store item scores item_score = {}  # Loop through items for i in similar_user_movies.columns:   # Get the ratings for movie i   movie_rating = similar_user_movies[i]   # Create a variable to store the score   total = 0   # Create a variable to store the number of scores   count = 0   # Loop through similar users   for u in similar_users.index:     # If the movie has rating     if pd.isna(movie_rating[u]) == False:       # Score is the sum of user similarity score multiply by the movie rating       score = similar_users[u] * movie_rating[u]       # Add the score to the total score for the movie so far       total += score       # Add 1 to the count       count +=1   # Get the average score for the item   item_score[i] = total / count  # Convert dictionary to pandas dataframe item_score = pd.DataFrame(item_score.items(), columns=['movie', 'movie_score'])      # Sort the movies by score ranked_item_score = item_score.sort_values(by='movie_score', ascending=False)  # Select top m movies m = 10 ranked_item_score.head(m) Out[21]: movie movie_score 16 Harry Potter and the Chamber of Secrets (2002) 1.888889 13 Eternal Sunshine of the Spotless Mind (2004) 1.888889 6 Bourne Identity, The (2002) 0.888889 29 Ocean's Eleven (2001) 0.888889 18 Inception (2010) 0.587491 3 Beautiful Mind, A (2001) 0.466667 5 Blade Runner (1982) 0.466667 12 Donnie Darko (2001) 0.466667 10 Departed, The (2006) 0.256727 31 Shawshank Redemption, The (1994) 0.222566 In\u00a0[22]: Copied! <pre># Average rating for the picked user\navg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]\n\n# Print the average movie rating for user 1\nprint(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}')\n</pre> # Average rating for the picked user avg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]  # Print the average movie rating for user 1 print(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}') <pre>The average movie rating for user 1 is 4.39\n</pre> <p>La calificaci\u00f3n promedio de la pel\u00edcula para el usuario 1 es 4.39, por lo que agregamos 4.39 nuevamente a la calificaci\u00f3n de la pel\u00edcula.</p> In\u00a0[23]: Copied! <pre># Calcuate the predicted rating\nranked_item_score['predicted_rating'] = ranked_item_score['movie_score'] + avg_rating\n\n# Take a look at the data\nranked_item_score.head(m)\n</pre> # Calcuate the predicted rating ranked_item_score['predicted_rating'] = ranked_item_score['movie_score'] + avg_rating  # Take a look at the data ranked_item_score.head(m) Out[23]: movie movie_score predicted_rating 16 Harry Potter and the Chamber of Secrets (2002) 1.888889 6.281746 13 Eternal Sunshine of the Spotless Mind (2004) 1.888889 6.281746 6 Bourne Identity, The (2002) 0.888889 5.281746 29 Ocean's Eleven (2001) 0.888889 5.281746 18 Inception (2010) 0.587491 4.980348 3 Beautiful Mind, A (2001) 0.466667 4.859524 5 Blade Runner (1982) 0.466667 4.859524 12 Donnie Darko (2001) 0.466667 4.859524 10 Departed, The (2006) 0.256727 4.649584 31 Shawshank Redemption, The (1994) 0.222566 4.615423 <p>Podemos ver que las 10 mejores pel\u00edculas recomendadas tienen calificaciones pronosticadas superiores a 4.5.</p>"},{"location":"blog/2022/2022-10-12-implicit/#collaborative-filtering","title":"Collaborative Filtering\u00b6","text":""},{"location":"blog/2022/2022-10-12-implicit/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Recommendation System: User-Based Collaborative Filtering o Filtrado Colaborativo Basado en el Usuario es un tipo de algoritmo de sistema de recomendaci\u00f3n que utiliza la similitud del usuario para hacer recomendaciones de productos.</p> <p>En este tutorial, hablaremos sobre</p> <ul> <li>\u00bfQu\u00e9 es el filtrado colaborativo basado en usuarios (usuario-usuario)?</li> <li>\u00bfC\u00f3mo crear una matriz usuario-producto?</li> <li>\u00bfC\u00f3mo procesar los datos para el filtrado colaborativo basado en el usuario?</li> <li>\u00bfC\u00f3mo identificar usuarios similares?</li> <li>\u00bfC\u00f3mo reducir el grupo de elementos?</li> <li>\u00bfC\u00f3mo clasificar los art\u00edculos para la recomendaci\u00f3n?</li> <li>\u00bfC\u00f3mo predecir la puntuaci\u00f3n de calificaci\u00f3n?</li> </ul> <p>Nota: Para entender a profundidad este algoritmo, se recomienda leer el documento oficial.</p>"},{"location":"blog/2022/2022-10-12-implicit/#algoritmo","title":"Algoritmo\u00b6","text":"<p>En primer lugar, comprendamos c\u00f3mo funciona el filtrado colaborativo basado en usuarios.</p> <p>El filtrado colaborativo basado en el usuario hace recomendaciones basadas en las interacciones del usuario con el producto en el pasado. La suposici\u00f3n detr\u00e1s del algoritmo es que a usuarios similares les gustan productos similares.</p> <p>El algoritmo de filtrado colaborativo basado en el usuario generalmente tiene los siguientes pasos:</p> <ol> <li>Encuentre usuarios similares en funci\u00f3n de las interacciones con elementos comunes.</li> <li>Identifique los elementos con una calificaci\u00f3n alta por parte de usuarios similares pero que no han sido expuestos al usuario activo de inter\u00e9s.</li> <li>Calcular la puntuaci\u00f3n media ponderada de cada elemento.</li> <li>Clasifique los elementos seg\u00fan la puntuaci\u00f3n y elija los n mejores elementos para recomendar.</li> </ol> <p></p>"},{"location":"blog/2022/2022-10-12-implicit/#importar-librerias","title":"Importar librer\u00edas\u00b6","text":"<p>En el primer paso, importaremos las bibliotecas de Python <code>pandas</code>, <code>numpy</code> y <code>scipy.stats</code>. Estas tres bibliotecas son para procesamiento de datos y c\u00e1lculos.</p> <p>Tambi\u00e9n importamos <code>seaborn</code> para la visualizaci\u00f3n y <code>cosine_similarity</code> para calcular el puntaje de similitud.</p>"},{"location":"blog/2022/2022-10-12-implicit/#descargar-y-leer-datos","title":"Descargar y leer datos\u00b6","text":"<p>Este tutorial utiliza el conjunto de datos movielens. Este conjunto de datos contiene calificaciones reales de usuarios de pel\u00edculas.</p> <p>Seguiremos los pasos a continuaci\u00f3n para obtener los conjuntos de datos:</p> <ol> <li>Vaya a https://grouplens.org/datasets/movielens/</li> <li>Descargue el conjunto de datos de 100k con el nombre de archivo \"ml-latest-small.zip\"</li> <li>Descomprima \"ml-latest-small.zip\"</li> <li>Copie la carpeta \"ml-latest-small\" en la carpeta de su proyecto</li> </ol> <p>Hay varios conjuntos de datos en la carpeta 100k movielens. Para este tutorial, utilizaremos dos clasificaciones y pel\u00edculas. Ahora vamos a leer en los datos de calificaci\u00f3n (<code>ratings.csv</code>).</p>"},{"location":"blog/2022/2022-10-12-implicit/#analisis-exploratorio-de-datos","title":"An\u00e1lisis exploratorio de datos\u00b6","text":"<p>Debemos filtrar las pel\u00edculas y mantener solo aquellas con m\u00e1s de 100 calificaciones para el an\u00e1lisis. Esto es para que el c\u00e1lculo sea manejable por la memoria de Google Colab.</p> <p>Para hacerlo, primero agrupamos las pel\u00edculas por t\u00edtulo, contamos el n\u00famero de calificaciones y mantenemos solo las pel\u00edculas con m\u00e1s de 100 calificaciones.</p> <p>Las calificaciones promedio de las pel\u00edculas tambi\u00e9n se calculan.</p> <p>Desde la salida <code>.info()</code>, podemos ver que quedan 134 pel\u00edculas.</p>"},{"location":"blog/2022/2022-10-12-implicit/#matriz-peliculas-de-usuario","title":"Matriz pel\u00edculas de usuario\u00b6","text":"<p>Transformaremos el conjunto de datos en un formato de matriz. Las filas de la matriz son usuarios y las columnas de la matriz son pel\u00edculas. El valor de la matriz es la calificaci\u00f3n de usuario de la pel\u00edcula si hay una calificaci\u00f3n. De lo contrario, muestra <code>NaN</code>.</p>"},{"location":"blog/2022/2022-10-12-implicit/#normalizacion-de-datos","title":"Normalizaci\u00f3n de datos\u00b6","text":"<p>Dado que algunas personas tienden a dar una calificaci\u00f3n m\u00e1s alta que otras, normalizamos la calificaci\u00f3n extrayendo la calificaci\u00f3n promedio de cada usuario.</p> <p>Despu\u00e9s de la normalizaci\u00f3n, las pel\u00edculas con una calificaci\u00f3n inferior a la calificaci\u00f3n promedio del usuario obtienen un valor negativo y las pel\u00edculas con una calificaci\u00f3n superior a la calificaci\u00f3n promedio del usuario obtienen un valor positivo.</p>"},{"location":"blog/2022/2022-10-12-implicit/#identifique-usuarios-similares","title":"Identifique Usuarios Similares\u00b6","text":"<p>Hay diferentes formas de medir las similitudes. La correlaci\u00f3n de Pearson y la similitud del coseno son dos m\u00e9todos ampliamente utilizados.</p> <p>En este tutorial, calcularemos la matriz de similitud del usuario utilizando la correlaci\u00f3n de Pearson.</p>"},{"location":"blog/2022/2022-10-12-implicit/#restringir-el-grupo-de-articulos","title":"Restringir el grupo de art\u00edculos\u00b6","text":"<p>Reduciremos el grupo de art\u00edculos haciendo lo siguiente:</p> <ul> <li>Elimine las pel\u00edculas que ha visto el usuario de destino (ID de usuario 1 en este ejemplo).</li> <li>Guarde solo las pel\u00edculas que otros usuarios similares hayan visto.</li> </ul> <p>Para eliminar las pel\u00edculas vistas por el usuario objetivo, mantenemos solo la fila para userId=1 en la matriz de elementos de usuario y eliminamos los elementos con valores faltantes.</p>"},{"location":"blog/2022/2022-10-12-implicit/#recomendar-articulos","title":"Recomendar art\u00edculos\u00b6","text":"<p>Decidiremos qu\u00e9 pel\u00edcula recomendar al usuario objetivo. Los elementos recomendados est\u00e1n determinados por el promedio ponderado del puntaje de similitud del usuario y la calificaci\u00f3n de la pel\u00edcula. Las calificaciones de las pel\u00edculas est\u00e1n ponderadas por las puntuaciones de similitud, por lo que los usuarios con mayor similitud obtienen una mayor ponderaci\u00f3n.</p> <p>Este c\u00f3digo recorre los elementos y los usuarios para obtener la puntuaci\u00f3n del elemento, clasificar la puntuaci\u00f3n de mayor a menor y elegir las 10 mejores pel\u00edculas para recomendar al ID de usuario 1.</p>"},{"location":"blog/2022/2022-10-12-implicit/#predecir-puntuaciones-opcional","title":"Predecir puntuaciones (opcional)\u00b6","text":"<p>Si el objetivo es elegir los elementos recomendados, basta con tener el rango de los elementos. Sin embargo, si el objetivo es predecir la calificaci\u00f3n del usuario, debemos sumar la calificaci\u00f3n promedio de la pel\u00edcula del usuario a la calificaci\u00f3n de la pel\u00edcula.</p>"},{"location":"blog/2022/2022-10-12-implicit/#resumen","title":"Resumen\u00b6","text":"<p>En este tutorial, analizamos c\u00f3mo crear un sistema de recomendaci\u00f3n de filtrado colaborativo basado en el usuario. Aprendiste</p> <ul> <li>\u00bfQu\u00e9 es el filtrado colaborativo basado en usuarios (usuario-usuario)?</li> <li>\u00bfC\u00f3mo crear una matriz usuario-producto?</li> <li>\u00bfC\u00f3mo procesar los datos para el filtrado colaborativo basado en el usuario?</li> <li>\u00bfC\u00f3mo identificar usuarios similares?</li> <li>\u00bfC\u00f3mo reducir el grupo de elementos?</li> <li>\u00bfC\u00f3mo clasificar los art\u00edculos para la recomendaci\u00f3n?</li> <li>\u00bfC\u00f3mo predecir la puntuaci\u00f3n de calificaci\u00f3n?</li> </ul>"},{"location":"blog/2022/2022-10-12-implicit/#referencias","title":"Referencias\u00b6","text":"<ul> <li>User-Based Collaborative Filtering.</li> <li>User-Based Collaborative Filtering In Python | Machine Learning.</li> <li>Collaborative Filtering : Data Science Concepts.</li> </ul>"},{"location":"blog/2023/hola/","title":"Prueba","text":"<pre><code># Dejame hacer algunas pruebas de codigo a ver\n</code></pre> bubble_sort.py<pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> Hola.py<pre><code># Haremos un print de una funcion \nprint('Hello World')\nif name == main \n</code></pre>"}]}